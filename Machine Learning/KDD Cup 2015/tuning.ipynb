{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('X_test.txt')\n",
    "X_train = pd.read_csv('X_train.txt')\n",
    "y_test = pd.read_csv('y_test.txt')\n",
    "y_train = pd.read_csv('y_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainos = pd.read_csv('X_trainos.txt')\n",
    "X_testos = pd.read_csv('X_testos.txt')\n",
    "y_testos = pd.read_csv('y_testos.txt')\n",
    "y_trainos = pd.read_csv('y_trainos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrollment_id</th>\n",
       "      <th>1pvLqtotBsKv7QSOsLicJDQMHx3lui6d</th>\n",
       "      <th>3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or</th>\n",
       "      <th>3cnZpv6ReApmCaZyaQwi2izDZxVRdC01</th>\n",
       "      <th>5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6</th>\n",
       "      <th>5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2</th>\n",
       "      <th>7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx</th>\n",
       "      <th>81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv</th>\n",
       "      <th>9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz</th>\n",
       "      <th>9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x</th>\n",
       "      <th>...</th>\n",
       "      <th>ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ</th>\n",
       "      <th>event_access</th>\n",
       "      <th>event_discussion</th>\n",
       "      <th>event_navigate</th>\n",
       "      <th>event_page_close</th>\n",
       "      <th>event_problem</th>\n",
       "      <th>event_wiki</th>\n",
       "      <th>source_browser</th>\n",
       "      <th>source_server</th>\n",
       "      <th>skip10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033069</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056546</td>\n",
       "      <td>0.036954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089095</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>0.117825</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.183223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313460</td>\n",
       "      <td>0.071161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045317</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.055931</td>\n",
       "      <td>0.020225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021148</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051629</td>\n",
       "      <td>0.010487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019951</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.021512</td>\n",
       "      <td>0.021723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012085</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011063</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043728</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.208459</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.079470</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.116165</td>\n",
       "      <td>0.055431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027057</td>\n",
       "      <td>0.020159</td>\n",
       "      <td>0.126888</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.018439</td>\n",
       "      <td>0.056679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027190</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.012907</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>0.066465</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.068433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>0.033458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035256</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.116779</td>\n",
       "      <td>0.045443</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024597</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.028698</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.077443</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039902</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.163142</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.067329</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.146281</td>\n",
       "      <td>0.048689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029790</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.075529</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.030905</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.087277</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021148</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043024</td>\n",
       "      <td>0.021223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.102719</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.068433</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.135833</td>\n",
       "      <td>0.057428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034982</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.138973</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>0.061810</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.107560</td>\n",
       "      <td>0.047940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044548</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.099698</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.050687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037715</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.117825</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033805</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.045317</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>0.022722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227931</td>\n",
       "      <td>0.125199</td>\n",
       "      <td>0.407855</td>\n",
       "      <td>0.5775</td>\n",
       "      <td>0.274834</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.545790</td>\n",
       "      <td>0.267915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054386</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.129909</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.041943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151813</td>\n",
       "      <td>0.053933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23983</th>\n",
       "      <td>200570</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23984</th>\n",
       "      <td>200572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.027190</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23985</th>\n",
       "      <td>200575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030211</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23986</th>\n",
       "      <td>200578</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23987</th>\n",
       "      <td>200616</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23988</th>\n",
       "      <td>200618</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23989</th>\n",
       "      <td>200636</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012085</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23990</th>\n",
       "      <td>200638</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23991</th>\n",
       "      <td>200659</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23992</th>\n",
       "      <td>200685</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.010449</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23993</th>\n",
       "      <td>200692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23994</th>\n",
       "      <td>200697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>200702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>200716</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>200731</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>200747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.021148</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>200755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>200762</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24001</th>\n",
       "      <td>200774</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002</th>\n",
       "      <td>200786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24003</th>\n",
       "      <td>200792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24004</th>\n",
       "      <td>200793</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24005</th>\n",
       "      <td>200801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24006</th>\n",
       "      <td>200811</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24007</th>\n",
       "      <td>200823</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24008</th>\n",
       "      <td>200832</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24009</th>\n",
       "      <td>200849</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24010</th>\n",
       "      <td>200867</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24011</th>\n",
       "      <td>200887</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24012</th>\n",
       "      <td>200889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24013 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       enrollment_id  1pvLqtotBsKv7QSOsLicJDQMHx3lui6d  \\\n",
       "0                 30                                 0   \n",
       "1                 31                                 0   \n",
       "2                 45                                 0   \n",
       "3                 48                                 0   \n",
       "4                 50                                 0   \n",
       "5                 68                                 0   \n",
       "6                 69                                 0   \n",
       "7                 75                                 0   \n",
       "8                 87                                 0   \n",
       "9                 92                                 0   \n",
       "10                97                                 0   \n",
       "11               101                                 0   \n",
       "12               104                                 0   \n",
       "13               111                                 0   \n",
       "14               113                                 0   \n",
       "15               119                                 0   \n",
       "16               123                                 0   \n",
       "17               129                                 0   \n",
       "18               143                                 0   \n",
       "19               151                                 0   \n",
       "20               164                                 0   \n",
       "21               166                                 0   \n",
       "22               167                                 0   \n",
       "23               171                                 0   \n",
       "24               180                                 0   \n",
       "25               193                                 0   \n",
       "26               195                                 0   \n",
       "27               219                                 0   \n",
       "28               223                                 0   \n",
       "29               228                                 0   \n",
       "...              ...                               ...   \n",
       "23983         200570                                 0   \n",
       "23984         200572                                 0   \n",
       "23985         200575                                 0   \n",
       "23986         200578                                 0   \n",
       "23987         200616                                 1   \n",
       "23988         200618                                 1   \n",
       "23989         200636                                 0   \n",
       "23990         200638                                 1   \n",
       "23991         200659                                 1   \n",
       "23992         200685                                 0   \n",
       "23993         200692                                 0   \n",
       "23994         200697                                 0   \n",
       "23995         200702                                 0   \n",
       "23996         200716                                 0   \n",
       "23997         200731                                 0   \n",
       "23998         200747                                 0   \n",
       "23999         200755                                 0   \n",
       "24000         200762                                 0   \n",
       "24001         200774                                 0   \n",
       "24002         200786                                 0   \n",
       "24003         200792                                 0   \n",
       "24004         200793                                 0   \n",
       "24005         200801                                 0   \n",
       "24006         200811                                 0   \n",
       "24007         200823                                 0   \n",
       "24008         200832                                 0   \n",
       "24009         200849                                 0   \n",
       "24010         200867                                 0   \n",
       "24011         200887                                 0   \n",
       "24012         200889                                 0   \n",
       "\n",
       "       3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or  3cnZpv6ReApmCaZyaQwi2izDZxVRdC01  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     0                                 0   \n",
       "3                                     0                                 0   \n",
       "4                                     0                                 0   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     0                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    0                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    0                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "23983                                 0                                 0   \n",
       "23984                                 0                                 0   \n",
       "23985                                 0                                 0   \n",
       "23986                                 0                                 0   \n",
       "23987                                 0                                 0   \n",
       "23988                                 0                                 0   \n",
       "23989                                 0                                 0   \n",
       "23990                                 0                                 0   \n",
       "23991                                 0                                 0   \n",
       "23992                                 0                                 0   \n",
       "23993                                 0                                 0   \n",
       "23994                                 0                                 0   \n",
       "23995                                 0                                 0   \n",
       "23996                                 0                                 0   \n",
       "23997                                 0                                 0   \n",
       "23998                                 0                                 0   \n",
       "23999                                 0                                 0   \n",
       "24000                                 0                                 0   \n",
       "24001                                 0                                 0   \n",
       "24002                                 0                                 0   \n",
       "24003                                 0                                 0   \n",
       "24004                                 0                                 0   \n",
       "24005                                 0                                 0   \n",
       "24006                                 0                                 0   \n",
       "24007                                 0                                 0   \n",
       "24008                                 0                                 0   \n",
       "24009                                 0                                 0   \n",
       "24010                                 0                                 0   \n",
       "24011                                 0                                 0   \n",
       "24012                                 0                                 0   \n",
       "\n",
       "       5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6  5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     0                                 0   \n",
       "3                                     0                                 0   \n",
       "4                                     0                                 0   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     0                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    0                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    0                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "23983                                 0                                 0   \n",
       "23984                                 0                                 0   \n",
       "23985                                 0                                 0   \n",
       "23986                                 0                                 0   \n",
       "23987                                 0                                 0   \n",
       "23988                                 0                                 0   \n",
       "23989                                 1                                 0   \n",
       "23990                                 0                                 0   \n",
       "23991                                 0                                 0   \n",
       "23992                                 0                                 0   \n",
       "23993                                 0                                 0   \n",
       "23994                                 0                                 0   \n",
       "23995                                 0                                 0   \n",
       "23996                                 0                                 0   \n",
       "23997                                 0                                 0   \n",
       "23998                                 0                                 0   \n",
       "23999                                 0                                 0   \n",
       "24000                                 0                                 0   \n",
       "24001                                 0                                 0   \n",
       "24002                                 0                                 0   \n",
       "24003                                 0                                 0   \n",
       "24004                                 0                                 0   \n",
       "24005                                 0                                 0   \n",
       "24006                                 0                                 0   \n",
       "24007                                 0                                 0   \n",
       "24008                                 0                                 0   \n",
       "24009                                 0                                 0   \n",
       "24010                                 0                                 0   \n",
       "24011                                 0                                 0   \n",
       "24012                                 0                                 0   \n",
       "\n",
       "       7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx  81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     1                                 0   \n",
       "3                                     0                                 0   \n",
       "4                                     1                                 0   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     0                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    0                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    1                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "23983                                 0                                 1   \n",
       "23984                                 0                                 1   \n",
       "23985                                 0                                 0   \n",
       "23986                                 0                                 1   \n",
       "23987                                 0                                 0   \n",
       "23988                                 0                                 0   \n",
       "23989                                 0                                 0   \n",
       "23990                                 0                                 0   \n",
       "23991                                 0                                 0   \n",
       "23992                                 0                                 1   \n",
       "23993                                 0                                 0   \n",
       "23994                                 0                                 1   \n",
       "23995                                 0                                 0   \n",
       "23996                                 0                                 1   \n",
       "23997                                 0                                 0   \n",
       "23998                                 0                                 1   \n",
       "23999                                 0                                 0   \n",
       "24000                                 0                                 1   \n",
       "24001                                 0                                 0   \n",
       "24002                                 1                                 0   \n",
       "24003                                 0                                 0   \n",
       "24004                                 0                                 0   \n",
       "24005                                 0                                 0   \n",
       "24006                                 0                                 0   \n",
       "24007                                 0                                 0   \n",
       "24008                                 0                                 0   \n",
       "24009                                 0                                 0   \n",
       "24010                                 0                                 0   \n",
       "24011                                 0                                 0   \n",
       "24012                                 0                                 0   \n",
       "\n",
       "       9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz  9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     0                                 0   \n",
       "3                                     0                                 0   \n",
       "4                                     0                                 0   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     0                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    0                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    0                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "23983                                 0                                 0   \n",
       "23984                                 0                                 0   \n",
       "23985                                 0                                 0   \n",
       "23986                                 0                                 0   \n",
       "23987                                 0                                 0   \n",
       "23988                                 0                                 0   \n",
       "23989                                 0                                 0   \n",
       "23990                                 0                                 0   \n",
       "23991                                 0                                 0   \n",
       "23992                                 0                                 0   \n",
       "23993                                 0                                 0   \n",
       "23994                                 0                                 0   \n",
       "23995                                 0                                 0   \n",
       "23996                                 0                                 0   \n",
       "23997                                 0                                 0   \n",
       "23998                                 0                                 0   \n",
       "23999                                 0                                 0   \n",
       "24000                                 0                                 0   \n",
       "24001                                 0                                 0   \n",
       "24002                                 0                                 0   \n",
       "24003                                 0                                 0   \n",
       "24004                                 0                                 0   \n",
       "24005                                 0                                 0   \n",
       "24006                                 0                                 0   \n",
       "24007                                 0                                 0   \n",
       "24008                                 0                                 0   \n",
       "24009                                 0                                 0   \n",
       "24010                                 0                                 0   \n",
       "24011                                 0                                 0   \n",
       "24012                                 0                                 0   \n",
       "\n",
       "        ...    ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ  event_access  \\\n",
       "0       ...                                   0      0.001093   \n",
       "1       ...                                   0      0.033069   \n",
       "2       ...                                   0      0.089095   \n",
       "3       ...                                   0      0.019131   \n",
       "4       ...                                   0      0.013392   \n",
       "5       ...                                   0      0.019951   \n",
       "6       ...                                   0      0.012025   \n",
       "7       ...                                   0      0.043728   \n",
       "8       ...                                   0      0.027057   \n",
       "9       ...                                   0      0.000000   \n",
       "10      ...                                   0      0.003280   \n",
       "11      ...                                   0      0.025690   \n",
       "12      ...                                   0      0.035256   \n",
       "13      ...                                   0      0.003280   \n",
       "14      ...                                   0      0.024597   \n",
       "15      ...                                   0      0.039902   \n",
       "16      ...                                   0      0.029790   \n",
       "17      ...                                   0      0.020771   \n",
       "18      ...                                   0      0.000000   \n",
       "19      ...                                   0      0.034709   \n",
       "20      ...                                   0      0.000820   \n",
       "21      ...                                   0      0.034982   \n",
       "22      ...                                   0      0.044548   \n",
       "23      ...                                   0      0.005466   \n",
       "24      ...                                   0      0.037715   \n",
       "25      ...                                   0      0.000820   \n",
       "26      ...                                   0      0.017764   \n",
       "27      ...                                   0      0.227931   \n",
       "28      ...                                   0      0.000000   \n",
       "29      ...                                   0      0.054386   \n",
       "...     ...                                 ...           ...   \n",
       "23983   ...                                   0      0.001366   \n",
       "23984   ...                                   0      0.004373   \n",
       "23985   ...                                   0      0.001093   \n",
       "23986   ...                                   0      0.000547   \n",
       "23987   ...                                   0      0.000000   \n",
       "23988   ...                                   0      0.000547   \n",
       "23989   ...                                   0      0.001093   \n",
       "23990   ...                                   0      0.000000   \n",
       "23991   ...                                   0      0.000000   \n",
       "23992   ...                                   0      0.003826   \n",
       "23993   ...                                   0      0.000000   \n",
       "23994   ...                                   0      0.001366   \n",
       "23995   ...                                   0      0.000273   \n",
       "23996   ...                                   0      0.002733   \n",
       "23997   ...                                   0      0.001093   \n",
       "23998   ...                                   0      0.003280   \n",
       "23999   ...                                   0      0.000000   \n",
       "24000   ...                                   0      0.000820   \n",
       "24001   ...                                   0      0.000000   \n",
       "24002   ...                                   0      0.000000   \n",
       "24003   ...                                   0      0.000000   \n",
       "24004   ...                                   0      0.000000   \n",
       "24005   ...                                   0      0.000000   \n",
       "24006   ...                                   0      0.000000   \n",
       "24007   ...                                   0      0.001093   \n",
       "24008   ...                                   0      0.000547   \n",
       "24009   ...                                   0      0.000000   \n",
       "24010   ...                                   0      0.000000   \n",
       "24011   ...                                   0      0.000820   \n",
       "24012   ...                                   0      0.000000   \n",
       "\n",
       "       event_discussion  event_navigate  event_page_close  event_problem  \\\n",
       "0              0.000000        0.003021            0.0075       0.002208   \n",
       "1              0.001592        0.048338            0.1425       0.020971   \n",
       "2              0.011936        0.117825            0.2875       0.183223   \n",
       "3              0.000000        0.045317            0.1025       0.019868   \n",
       "4              0.000000        0.021148            0.0550       0.052980   \n",
       "5              0.000796        0.048338            0.0225       0.020971   \n",
       "6              0.000000        0.012085            0.0200       0.004415   \n",
       "7              0.001061        0.208459            0.1800       0.079470   \n",
       "8              0.020159        0.126888            0.0075       0.033113   \n",
       "9              0.000000        0.003021            0.0000       0.000000   \n",
       "10             0.000000        0.027190            0.0025       0.011038   \n",
       "11             0.009019        0.066465            0.0125       0.068433   \n",
       "12             0.001061        0.175227            0.2275       0.033113   \n",
       "13             0.000000        0.084592            0.0100       0.000000   \n",
       "14             0.004244        0.042296            0.1575       0.028698   \n",
       "15             0.002387        0.163142            0.2700       0.067329   \n",
       "16             0.006897        0.075529            0.1900       0.030905   \n",
       "17             0.000000        0.021148            0.1025       0.009934   \n",
       "18             0.000000        0.006042            0.0000       0.000000   \n",
       "19             0.022812        0.102719            0.2300       0.068433   \n",
       "20             0.000000        0.009063            0.0000       0.000000   \n",
       "21             0.006897        0.138973            0.1850       0.061810   \n",
       "22             0.003714        0.099698            0.1850       0.019868   \n",
       "23             0.000000        0.024169            0.0250       0.000000   \n",
       "24             0.001857        0.117825            0.0225       0.020971   \n",
       "25             0.000000        0.015106            0.0050       0.000000   \n",
       "26             0.002122        0.045317            0.0600       0.008830   \n",
       "27             0.125199        0.407855            0.5775       0.274834   \n",
       "28             0.000000        0.006042            0.0000       0.000000   \n",
       "29             0.000796        0.129909            0.2875       0.041943   \n",
       "...                 ...             ...               ...            ...   \n",
       "23983          0.000000        0.015106            0.0025       0.008830   \n",
       "23984          0.000531        0.027190            0.0025       0.002208   \n",
       "23985          0.000000        0.030211            0.0075       0.000000   \n",
       "23986          0.000000        0.006042            0.0025       0.000000   \n",
       "23987          0.000000        0.003021            0.0000       0.000000   \n",
       "23988          0.000000        0.006042            0.0025       0.000000   \n",
       "23989          0.000000        0.012085            0.0050       0.000000   \n",
       "23990          0.000000        0.015106            0.0000       0.000000   \n",
       "23991          0.000000        0.003021            0.0000       0.000000   \n",
       "23992          0.000531        0.015106            0.0000       0.002208   \n",
       "23993          0.000000        0.006042            0.0000       0.000000   \n",
       "23994          0.000000        0.003021            0.0025       0.000000   \n",
       "23995          0.000000        0.009063            0.0025       0.000000   \n",
       "23996          0.000000        0.015106            0.0125       0.000000   \n",
       "23997          0.000000        0.006042            0.0050       0.000000   \n",
       "23998          0.000531        0.021148            0.0125       0.011038   \n",
       "23999          0.000000        0.003021            0.0000       0.000000   \n",
       "24000          0.000000        0.003021            0.0025       0.000000   \n",
       "24001          0.000000        0.003021            0.0000       0.000000   \n",
       "24002          0.000000        0.003021            0.0000       0.000000   \n",
       "24003          0.000000        0.006042            0.0000       0.000000   \n",
       "24004          0.000000        0.003021            0.0000       0.000000   \n",
       "24005          0.000000        0.003021            0.0000       0.000000   \n",
       "24006          0.000000        0.003021            0.0000       0.000000   \n",
       "24007          0.000000        0.009063            0.0000       0.000000   \n",
       "24008          0.000000        0.003021            0.0025       0.000000   \n",
       "24009          0.000000        0.006042            0.0000       0.000000   \n",
       "24010          0.000000        0.003021            0.0000       0.000000   \n",
       "24011          0.000000        0.006042            0.0025       0.000000   \n",
       "24012          0.000000        0.003021            0.0000       0.000000   \n",
       "\n",
       "       event_wiki  source_browser  source_server  skip10  \n",
       "0        0.000000        0.002459       0.001498       0  \n",
       "1        0.000000        0.056546       0.036954       0  \n",
       "2        0.000000        0.313460       0.071161       0  \n",
       "3        0.004167        0.055931       0.020225       0  \n",
       "4        0.000000        0.051629       0.010487       1  \n",
       "5        0.004167        0.021512       0.021723       0  \n",
       "6        0.000000        0.011063       0.012235       0  \n",
       "7        0.004167        0.116165       0.055431       0  \n",
       "8        0.020833        0.018439       0.056679       0  \n",
       "9        0.000000        0.000000       0.000250       0  \n",
       "10       0.004167        0.012907       0.003995       0  \n",
       "11       0.000000        0.052858       0.033458       0  \n",
       "12       0.020833        0.116779       0.045443       0  \n",
       "13       0.004167        0.004917       0.010237       0  \n",
       "14       0.012500        0.077443       0.030961       0  \n",
       "15       0.020833        0.146281       0.048689       0  \n",
       "16       0.004167        0.087277       0.039700       0  \n",
       "17       0.000000        0.043024       0.021223       0  \n",
       "18       0.000000        0.000000       0.000499       0  \n",
       "19       0.012500        0.135833       0.057428       0  \n",
       "20       0.004167        0.000000       0.001748       0  \n",
       "21       0.004167        0.107560       0.047940       0  \n",
       "22       0.025000        0.097726       0.050687       0  \n",
       "23       0.000000        0.009219       0.006991       0  \n",
       "24       0.000000        0.033805       0.040200       0  \n",
       "25       0.000000        0.002459       0.001998       0  \n",
       "26       0.000000        0.027044       0.022722       0  \n",
       "27       0.058333        0.545790       0.267915       0  \n",
       "28       0.000000        0.000000       0.000499       0  \n",
       "29       0.000000        0.151813       0.053933       0  \n",
       "...           ...             ...            ...     ...  \n",
       "23983    0.004167        0.007990       0.002247       1  \n",
       "23984    0.008333        0.009219       0.004494       1  \n",
       "23985    0.004167        0.003688       0.003745       0  \n",
       "23986    0.000000        0.001229       0.000999       1  \n",
       "23987    0.000000        0.000000       0.000250       0  \n",
       "23988    0.000000        0.001229       0.000999       0  \n",
       "23989    0.000000        0.001844       0.001998       1  \n",
       "23990    0.000000        0.000000       0.001248       0  \n",
       "23991    0.000000        0.000000       0.000250       0  \n",
       "23992    0.004167        0.010449       0.003246       1  \n",
       "23993    0.000000        0.000000       0.000499       0  \n",
       "23994    0.000000        0.003073       0.000749       1  \n",
       "23995    0.000000        0.000615       0.000999       1  \n",
       "23996    0.000000        0.006761       0.003246       1  \n",
       "23997    0.000000        0.001844       0.001498       0  \n",
       "23998    0.000000        0.012293       0.004744       1  \n",
       "23999    0.000000        0.000000       0.000250       1  \n",
       "24000    0.000000        0.000615       0.000999       1  \n",
       "24001    0.000000        0.000000       0.000250       1  \n",
       "24002    0.000000        0.000000       0.000250       0  \n",
       "24003    0.000000        0.000000       0.000499       0  \n",
       "24004    0.000000        0.000000       0.000250       1  \n",
       "24005    0.000000        0.000000       0.000250       0  \n",
       "24006    0.000000        0.000000       0.000250       1  \n",
       "24007    0.000000        0.000000       0.001748       1  \n",
       "24008    0.000000        0.000615       0.000749       0  \n",
       "24009    0.000000        0.000000       0.000499       0  \n",
       "24010    0.000000        0.000000       0.000250       0  \n",
       "24011    0.000000        0.001229       0.001248       0  \n",
       "24012    0.000000        0.000000       0.000250       0  \n",
       "\n",
       "[24013 rows x 49 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72365</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72366</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72367</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72368</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72369</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72370</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72371</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72372</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72373</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72374</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72375</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72376</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72377</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72378</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72379</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72380</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72381</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72382</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72383</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72384</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72385</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72386</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72387</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72388</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72389</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72390</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72391</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72392</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72393</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72394</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72395 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Drop\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         1\n",
       "4         0\n",
       "5         1\n",
       "6         0\n",
       "7         0\n",
       "8         1\n",
       "9         0\n",
       "10        0\n",
       "11        1\n",
       "12        0\n",
       "13        1\n",
       "14        1\n",
       "15        0\n",
       "16        0\n",
       "17        0\n",
       "18        0\n",
       "19        0\n",
       "20        1\n",
       "21        1\n",
       "22        0\n",
       "23        1\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "...     ...\n",
       "72365     1\n",
       "72366     1\n",
       "72367     1\n",
       "72368     1\n",
       "72369     1\n",
       "72370     1\n",
       "72371     0\n",
       "72372     1\n",
       "72373     1\n",
       "72374     1\n",
       "72375     1\n",
       "72376     1\n",
       "72377     1\n",
       "72378     1\n",
       "72379     1\n",
       "72380     1\n",
       "72381     1\n",
       "72382     1\n",
       "72383     1\n",
       "72384     1\n",
       "72385     0\n",
       "72386     1\n",
       "72387     0\n",
       "72388     1\n",
       "72389     1\n",
       "72390     1\n",
       "72391     1\n",
       "72392     1\n",
       "72393     1\n",
       "72394     1\n",
       "\n",
       "[72395 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrollment_id</th>\n",
       "      <th>1pvLqtotBsKv7QSOsLicJDQMHx3lui6d</th>\n",
       "      <th>3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or</th>\n",
       "      <th>3cnZpv6ReApmCaZyaQwi2izDZxVRdC01</th>\n",
       "      <th>5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6</th>\n",
       "      <th>5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2</th>\n",
       "      <th>7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx</th>\n",
       "      <th>81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv</th>\n",
       "      <th>9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz</th>\n",
       "      <th>9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x</th>\n",
       "      <th>...</th>\n",
       "      <th>ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ</th>\n",
       "      <th>event_access</th>\n",
       "      <th>event_discussion</th>\n",
       "      <th>event_navigate</th>\n",
       "      <th>event_page_close</th>\n",
       "      <th>event_problem</th>\n",
       "      <th>event_wiki</th>\n",
       "      <th>source_browser</th>\n",
       "      <th>source_server</th>\n",
       "      <th>skip10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038521</td>\n",
       "      <td>0.169666</td>\n",
       "      <td>0.085799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085265</td>\n",
       "      <td>0.023179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.104485</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>0.046225</td>\n",
       "      <td>0.223650</td>\n",
       "      <td>0.167653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188456</td>\n",
       "      <td>0.039346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093851</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.030817</td>\n",
       "      <td>0.154242</td>\n",
       "      <td>0.092702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142108</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092464</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.064267</td>\n",
       "      <td>0.147929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137298</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030817</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041609</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>0.046272</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027277</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.053929</td>\n",
       "      <td>0.087404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.020114</td>\n",
       "      <td>0.018699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.097686</td>\n",
       "      <td>0.063116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.009544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018365</td>\n",
       "      <td>0.013829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067499</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>0.156812</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.041131</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>0.007986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037910</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.070878</td>\n",
       "      <td>0.113111</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.040665</td>\n",
       "      <td>0.026685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041609</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.052908</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155802</td>\n",
       "      <td>0.303670</td>\n",
       "      <td>0.246533</td>\n",
       "      <td>0.439589</td>\n",
       "      <td>0.067061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140359</td>\n",
       "      <td>0.347487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.011697</td>\n",
       "      <td>0.069337</td>\n",
       "      <td>0.293059</td>\n",
       "      <td>0.033531</td>\n",
       "      <td>0.008876</td>\n",
       "      <td>0.099257</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042996</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>0.073960</td>\n",
       "      <td>0.159383</td>\n",
       "      <td>0.019724</td>\n",
       "      <td>0.014793</td>\n",
       "      <td>0.050721</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110495</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.073960</td>\n",
       "      <td>0.218509</td>\n",
       "      <td>0.139053</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.137735</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030975</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.040062</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.039448</td>\n",
       "      <td>0.008876</td>\n",
       "      <td>0.020988</td>\n",
       "      <td>0.020257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>0.066838</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045307</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.057011</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.040665</td>\n",
       "      <td>0.029607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.030817</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026194</td>\n",
       "      <td>0.100257</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031482</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.063174</td>\n",
       "      <td>0.231362</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.029586</td>\n",
       "      <td>0.055094</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.033419</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.005649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050393</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>0.190231</td>\n",
       "      <td>0.039448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059904</td>\n",
       "      <td>0.027269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.038521</td>\n",
       "      <td>0.136247</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>0.014793</td>\n",
       "      <td>0.041102</td>\n",
       "      <td>0.026295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.007791</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72365</th>\n",
       "      <td>200821</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72366</th>\n",
       "      <td>200824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72367</th>\n",
       "      <td>200826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72368</th>\n",
       "      <td>200828</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72369</th>\n",
       "      <td>200830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72370</th>\n",
       "      <td>200837</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72371</th>\n",
       "      <td>200843</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009245</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72372</th>\n",
       "      <td>200846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72373</th>\n",
       "      <td>200848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72374</th>\n",
       "      <td>200850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72375</th>\n",
       "      <td>200851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72376</th>\n",
       "      <td>200859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72377</th>\n",
       "      <td>200860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72378</th>\n",
       "      <td>200862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72379</th>\n",
       "      <td>200866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72380</th>\n",
       "      <td>200868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72381</th>\n",
       "      <td>200873</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72382</th>\n",
       "      <td>200875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72383</th>\n",
       "      <td>200876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72384</th>\n",
       "      <td>200879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72385</th>\n",
       "      <td>200880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72386</th>\n",
       "      <td>200881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72387</th>\n",
       "      <td>200882</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72388</th>\n",
       "      <td>200883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72389</th>\n",
       "      <td>200885</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72390</th>\n",
       "      <td>200888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72391</th>\n",
       "      <td>200895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72392</th>\n",
       "      <td>200897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72393</th>\n",
       "      <td>200901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72394</th>\n",
       "      <td>200904</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72395 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       enrollment_id  1pvLqtotBsKv7QSOsLicJDQMHx3lui6d  \\\n",
       "0                  1                                 0   \n",
       "1                  4                                 0   \n",
       "2                  5                                 0   \n",
       "3                  7                                 0   \n",
       "4                 13                                 0   \n",
       "5                 14                                 0   \n",
       "6                 18                                 0   \n",
       "7                 20                                 0   \n",
       "8                 22                                 0   \n",
       "9                 23                                 0   \n",
       "10                26                                 0   \n",
       "11                28                                 0   \n",
       "12                35                                 0   \n",
       "13                39                                 0   \n",
       "14                46                                 0   \n",
       "15                49                                 0   \n",
       "16                53                                 0   \n",
       "17                55                                 0   \n",
       "18                58                                 0   \n",
       "19                60                                 0   \n",
       "20                65                                 0   \n",
       "21                67                                 0   \n",
       "22                70                                 0   \n",
       "23                73                                 0   \n",
       "24                74                                 0   \n",
       "25                76                                 0   \n",
       "26                80                                 0   \n",
       "27                81                                 0   \n",
       "28                82                                 0   \n",
       "29                83                                 0   \n",
       "...              ...                               ...   \n",
       "72365         200821                                 0   \n",
       "72366         200824                                 0   \n",
       "72367         200826                                 0   \n",
       "72368         200828                                 0   \n",
       "72369         200830                                 0   \n",
       "72370         200837                                 0   \n",
       "72371         200843                                 0   \n",
       "72372         200846                                 0   \n",
       "72373         200848                                 0   \n",
       "72374         200850                                 0   \n",
       "72375         200851                                 0   \n",
       "72376         200859                                 0   \n",
       "72377         200860                                 0   \n",
       "72378         200862                                 0   \n",
       "72379         200866                                 0   \n",
       "72380         200868                                 0   \n",
       "72381         200873                                 0   \n",
       "72382         200875                                 0   \n",
       "72383         200876                                 0   \n",
       "72384         200879                                 0   \n",
       "72385         200880                                 0   \n",
       "72386         200881                                 0   \n",
       "72387         200882                                 0   \n",
       "72388         200883                                 0   \n",
       "72389         200885                                 0   \n",
       "72390         200888                                 0   \n",
       "72391         200895                                 0   \n",
       "72392         200897                                 0   \n",
       "72393         200901                                 0   \n",
       "72394         200904                                 0   \n",
       "\n",
       "       3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or  3cnZpv6ReApmCaZyaQwi2izDZxVRdC01  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     0                                 0   \n",
       "3                                     0                                 0   \n",
       "4                                     0                                 0   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     0                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    0                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    0                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "72365                                 0                                 0   \n",
       "72366                                 0                                 0   \n",
       "72367                                 0                                 0   \n",
       "72368                                 0                                 0   \n",
       "72369                                 0                                 0   \n",
       "72370                                 0                                 0   \n",
       "72371                                 0                                 0   \n",
       "72372                                 0                                 0   \n",
       "72373                                 0                                 0   \n",
       "72374                                 0                                 0   \n",
       "72375                                 0                                 0   \n",
       "72376                                 0                                 0   \n",
       "72377                                 0                                 0   \n",
       "72378                                 0                                 0   \n",
       "72379                                 0                                 0   \n",
       "72380                                 0                                 0   \n",
       "72381                                 0                                 0   \n",
       "72382                                 0                                 0   \n",
       "72383                                 0                                 0   \n",
       "72384                                 0                                 0   \n",
       "72385                                 0                                 0   \n",
       "72386                                 0                                 0   \n",
       "72387                                 0                                 0   \n",
       "72388                                 0                                 0   \n",
       "72389                                 0                                 0   \n",
       "72390                                 0                                 0   \n",
       "72391                                 0                                 0   \n",
       "72392                                 0                                 0   \n",
       "72393                                 0                                 0   \n",
       "72394                                 0                                 0   \n",
       "\n",
       "       5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6  5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     0                                 0   \n",
       "3                                     0                                 0   \n",
       "4                                     0                                 1   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     0                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    0                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    0                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "72365                                 0                                 0   \n",
       "72366                                 0                                 0   \n",
       "72367                                 0                                 0   \n",
       "72368                                 0                                 0   \n",
       "72369                                 0                                 0   \n",
       "72370                                 0                                 0   \n",
       "72371                                 0                                 0   \n",
       "72372                                 0                                 0   \n",
       "72373                                 0                                 0   \n",
       "72374                                 0                                 0   \n",
       "72375                                 0                                 0   \n",
       "72376                                 0                                 0   \n",
       "72377                                 0                                 0   \n",
       "72378                                 0                                 0   \n",
       "72379                                 0                                 0   \n",
       "72380                                 0                                 0   \n",
       "72381                                 0                                 0   \n",
       "72382                                 0                                 0   \n",
       "72383                                 0                                 0   \n",
       "72384                                 0                                 0   \n",
       "72385                                 0                                 0   \n",
       "72386                                 0                                 0   \n",
       "72387                                 0                                 0   \n",
       "72388                                 0                                 0   \n",
       "72389                                 0                                 0   \n",
       "72390                                 0                                 0   \n",
       "72391                                 0                                 0   \n",
       "72392                                 0                                 0   \n",
       "72393                                 0                                 0   \n",
       "72394                                 0                                 0   \n",
       "\n",
       "       7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx  81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     1                                 0   \n",
       "3                                     1                                 0   \n",
       "4                                     0                                 0   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     1                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    1                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    1                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    0                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "72365                                 0                                 0   \n",
       "72366                                 0                                 0   \n",
       "72367                                 1                                 0   \n",
       "72368                                 0                                 0   \n",
       "72369                                 0                                 0   \n",
       "72370                                 0                                 0   \n",
       "72371                                 0                                 0   \n",
       "72372                                 0                                 0   \n",
       "72373                                 0                                 0   \n",
       "72374                                 0                                 0   \n",
       "72375                                 0                                 0   \n",
       "72376                                 0                                 0   \n",
       "72377                                 0                                 0   \n",
       "72378                                 0                                 0   \n",
       "72379                                 0                                 0   \n",
       "72380                                 0                                 0   \n",
       "72381                                 0                                 0   \n",
       "72382                                 0                                 0   \n",
       "72383                                 0                                 0   \n",
       "72384                                 0                                 0   \n",
       "72385                                 0                                 0   \n",
       "72386                                 0                                 0   \n",
       "72387                                 0                                 0   \n",
       "72388                                 0                                 0   \n",
       "72389                                 0                                 0   \n",
       "72390                                 0                                 0   \n",
       "72391                                 0                                 0   \n",
       "72392                                 0                                 0   \n",
       "72393                                 0                                 0   \n",
       "72394                                 0                                 0   \n",
       "\n",
       "       9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz  9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x  \\\n",
       "0                                     0                                 0   \n",
       "1                                     0                                 0   \n",
       "2                                     0                                 0   \n",
       "3                                     0                                 0   \n",
       "4                                     0                                 0   \n",
       "5                                     0                                 0   \n",
       "6                                     0                                 0   \n",
       "7                                     0                                 0   \n",
       "8                                     0                                 0   \n",
       "9                                     0                                 0   \n",
       "10                                    0                                 0   \n",
       "11                                    0                                 0   \n",
       "12                                    0                                 0   \n",
       "13                                    0                                 0   \n",
       "14                                    0                                 0   \n",
       "15                                    0                                 0   \n",
       "16                                    0                                 0   \n",
       "17                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "19                                    0                                 0   \n",
       "20                                    0                                 0   \n",
       "21                                    0                                 0   \n",
       "22                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "24                                    0                                 0   \n",
       "25                                    0                                 0   \n",
       "26                                    0                                 0   \n",
       "27                                    0                                 0   \n",
       "28                                    0                                 0   \n",
       "29                                    0                                 0   \n",
       "...                                 ...                               ...   \n",
       "72365                                 0                                 0   \n",
       "72366                                 0                                 0   \n",
       "72367                                 0                                 0   \n",
       "72368                                 0                                 0   \n",
       "72369                                 0                                 0   \n",
       "72370                                 0                                 0   \n",
       "72371                                 0                                 0   \n",
       "72372                                 0                                 0   \n",
       "72373                                 0                                 0   \n",
       "72374                                 0                                 0   \n",
       "72375                                 0                                 0   \n",
       "72376                                 0                                 0   \n",
       "72377                                 0                                 0   \n",
       "72378                                 0                                 0   \n",
       "72379                                 0                                 0   \n",
       "72380                                 0                                 0   \n",
       "72381                                 0                                 0   \n",
       "72382                                 0                                 0   \n",
       "72383                                 0                                 0   \n",
       "72384                                 0                                 0   \n",
       "72385                                 0                                 0   \n",
       "72386                                 0                                 0   \n",
       "72387                                 0                                 0   \n",
       "72388                                 0                                 0   \n",
       "72389                                 0                                 0   \n",
       "72390                                 0                                 0   \n",
       "72391                                 0                                 0   \n",
       "72392                                 0                                 0   \n",
       "72393                                 0                                 0   \n",
       "72394                                 0                                 0   \n",
       "\n",
       "        ...    ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ  event_access  \\\n",
       "0       ...                                   0      0.049468   \n",
       "1       ...                                   0      0.029589   \n",
       "2       ...                                   0      0.104485   \n",
       "3       ...                                   0      0.093851   \n",
       "4       ...                                   0      0.092464   \n",
       "5       ...                                   0      0.027739   \n",
       "6       ...                                   0      0.041609   \n",
       "7       ...                                   0      0.027277   \n",
       "8       ...                                   0      0.030513   \n",
       "9       ...                                   0      0.024503   \n",
       "10      ...                                   0      0.067499   \n",
       "11      ...                                   0      0.020804   \n",
       "12      ...                                   0      0.037910   \n",
       "13      ...                                   0      0.041609   \n",
       "14      ...                                   0      0.000000   \n",
       "15      ...                                   0      0.155802   \n",
       "16      ...                                   0      0.077670   \n",
       "17      ...                                   0      0.042996   \n",
       "18      ...                                   0      0.110495   \n",
       "19      ...                                   0      0.030975   \n",
       "20      ...                                   0      0.014794   \n",
       "21      ...                                   0      0.045307   \n",
       "22      ...                                   0      0.029126   \n",
       "23      ...                                   0      0.028202   \n",
       "24      ...                                   0      0.067961   \n",
       "25      ...                                   0      0.029126   \n",
       "26      ...                                   0      0.010171   \n",
       "27      ...                                   0      0.050393   \n",
       "28      ...                                   0      0.031438   \n",
       "29      ...                                   0      0.011096   \n",
       "...     ...                                 ...           ...   \n",
       "72365   ...                                   0      0.000000   \n",
       "72366   ...                                   0      0.000000   \n",
       "72367   ...                                   0      0.000000   \n",
       "72368   ...                                   0      0.000000   \n",
       "72369   ...                                   0      0.001849   \n",
       "72370   ...                                   0      0.000000   \n",
       "72371   ...                                   0      0.004161   \n",
       "72372   ...                                   0      0.000000   \n",
       "72373   ...                                   0      0.000000   \n",
       "72374   ...                                   0      0.001849   \n",
       "72375   ...                                   0      0.000000   \n",
       "72376   ...                                   0      0.000000   \n",
       "72377   ...                                   0      0.000000   \n",
       "72378   ...                                   0      0.000000   \n",
       "72379   ...                                   0      0.000000   \n",
       "72380   ...                                   0      0.000000   \n",
       "72381   ...                                   0      0.000925   \n",
       "72382   ...                                   0      0.000462   \n",
       "72383   ...                                   0      0.000000   \n",
       "72384   ...                                   0      0.000000   \n",
       "72385   ...                                   0      0.000925   \n",
       "72386   ...                                   0      0.000000   \n",
       "72387   ...                                   0      0.000000   \n",
       "72388   ...                                   0      0.000925   \n",
       "72389   ...                                   0      0.000000   \n",
       "72390   ...                                   0      0.000925   \n",
       "72391   ...                                   0      0.000000   \n",
       "72392   ...                                   0      0.000000   \n",
       "72393   ...                                   0      0.001387   \n",
       "72394   ...                                   0      0.000000   \n",
       "\n",
       "       event_discussion  event_navigate  event_page_close  event_problem  \\\n",
       "0              0.000000        0.038521          0.169666       0.085799   \n",
       "1              0.000000        0.023112          0.025707       0.005917   \n",
       "2              0.007798        0.046225          0.223650       0.167653   \n",
       "3              0.007569        0.030817          0.154242       0.092702   \n",
       "4              0.000917        0.023112          0.064267       0.147929   \n",
       "5              0.000000        0.030817          0.012853       0.011834   \n",
       "6              0.000688        0.021572          0.046272       0.016765   \n",
       "7              0.000229        0.053929          0.087404       0.000000   \n",
       "8              0.000000        0.001541          0.097686       0.063116   \n",
       "9              0.000000        0.027735          0.071979       0.000000   \n",
       "10             0.000229        0.032357          0.156812       0.004931   \n",
       "11             0.000000        0.016949          0.041131       0.011834   \n",
       "12             0.002982        0.070878          0.113111       0.037475   \n",
       "13             0.000688        0.032357          0.192802       0.012821   \n",
       "14             0.000000        0.001541          0.000000       0.000000   \n",
       "15             0.303670        0.246533          0.439589       0.067061   \n",
       "16             0.011697        0.069337          0.293059       0.033531   \n",
       "17             0.005275        0.073960          0.159383       0.019724   \n",
       "18             0.009174        0.073960          0.218509       0.139053   \n",
       "19             0.002752        0.040062          0.010283       0.039448   \n",
       "20             0.000000        0.013867          0.066838       0.001972   \n",
       "21             0.005046        0.057011          0.149100       0.007890   \n",
       "22             0.003670        0.030817          0.017995       0.017751   \n",
       "23             0.000000        0.026194          0.100257       0.028600   \n",
       "24             0.005734        0.063174          0.231362       0.002959   \n",
       "25             0.000000        0.010786          0.033419       0.025641   \n",
       "26             0.000000        0.007704          0.002571       0.007890   \n",
       "27             0.000917        0.032357          0.190231       0.039448   \n",
       "28             0.007569        0.038521          0.136247       0.017751   \n",
       "29             0.000000        0.023112          0.020566       0.001972   \n",
       "...                 ...             ...               ...            ...   \n",
       "72365          0.000000        0.001541          0.000000       0.000000   \n",
       "72366          0.000000        0.001541          0.000000       0.000000   \n",
       "72367          0.000000        0.003082          0.000000       0.000000   \n",
       "72368          0.000000        0.003082          0.000000       0.000000   \n",
       "72369          0.000000        0.003082          0.005141       0.000000   \n",
       "72370          0.000000        0.004622          0.000000       0.000000   \n",
       "72371          0.000000        0.009245          0.002571       0.000000   \n",
       "72372          0.000000        0.001541          0.000000       0.000000   \n",
       "72373          0.000000        0.001541          0.000000       0.000000   \n",
       "72374          0.000000        0.001541          0.007712       0.000000   \n",
       "72375          0.000000        0.001541          0.000000       0.000000   \n",
       "72376          0.000000        0.003082          0.000000       0.000000   \n",
       "72377          0.000000        0.001541          0.000000       0.000000   \n",
       "72378          0.000000        0.001541          0.000000       0.000000   \n",
       "72379          0.000000        0.001541          0.000000       0.000000   \n",
       "72380          0.000000        0.001541          0.000000       0.000000   \n",
       "72381          0.000000        0.007704          0.002571       0.000000   \n",
       "72382          0.000000        0.010786          0.002571       0.000000   \n",
       "72383          0.000000        0.003082          0.000000       0.000000   \n",
       "72384          0.000000        0.007704          0.000000       0.000000   \n",
       "72385          0.000000        0.003082          0.000000       0.000000   \n",
       "72386          0.000000        0.001541          0.000000       0.000000   \n",
       "72387          0.000917        0.006163          0.000000       0.000000   \n",
       "72388          0.000000        0.003082          0.002571       0.000000   \n",
       "72389          0.000000        0.001541          0.000000       0.000000   \n",
       "72390          0.000229        0.007704          0.002571       0.000000   \n",
       "72391          0.000000        0.001541          0.000000       0.000000   \n",
       "72392          0.000000        0.001541          0.000000       0.000000   \n",
       "72393          0.000000        0.003082          0.005141       0.000000   \n",
       "72394          0.000000        0.001541          0.000000       0.000000   \n",
       "\n",
       "       event_wiki  source_browser  source_server  skip10  \n",
       "0        0.000000        0.085265       0.023179       0  \n",
       "1        0.000000        0.008308       0.015582       0  \n",
       "2        0.000000        0.188456       0.039346       0  \n",
       "3        0.000000        0.142108       0.029996       0  \n",
       "4        0.000000        0.137298       0.029022       1  \n",
       "5        0.000000        0.008308       0.016167       0  \n",
       "6        0.002959        0.020551       0.020647       1  \n",
       "7        0.002959        0.020114       0.018699       0  \n",
       "8        0.000000        0.068212       0.009544       1  \n",
       "9        0.000000        0.018365       0.013829       0  \n",
       "10       0.000000        0.044600       0.031749       0  \n",
       "11       0.000000        0.027984       0.007986       0  \n",
       "12       0.002959        0.040665       0.026685       0  \n",
       "13       0.005917        0.052908       0.022594       0  \n",
       "14       0.000000        0.000000       0.000195       1  \n",
       "15       0.000000        0.140359       0.347487       0  \n",
       "16       0.008876        0.099257       0.049669       0  \n",
       "17       0.014793        0.050721       0.031944       0  \n",
       "18       0.005917        0.137735       0.053370       0  \n",
       "19       0.008876        0.020988       0.020257       0  \n",
       "20       0.002959        0.020551       0.008376       0  \n",
       "21       0.005917        0.040665       0.029607       0  \n",
       "22       0.002959        0.010931       0.020062       0  \n",
       "23       0.000000        0.031482       0.014414       0  \n",
       "24       0.029586        0.055094       0.043046       0  \n",
       "25       0.000000        0.017490       0.014608       0  \n",
       "26       0.000000        0.003935       0.005649       0  \n",
       "27       0.000000        0.059904       0.027269       0  \n",
       "28       0.014793        0.041102       0.026295       1  \n",
       "29       0.000000        0.006122       0.007791       0  \n",
       "...           ...             ...            ...     ...  \n",
       "72365    0.000000        0.000000       0.000195       0  \n",
       "72366    0.000000        0.000000       0.000195       1  \n",
       "72367    0.000000        0.000000       0.000390       0  \n",
       "72368    0.000000        0.000000       0.000390       1  \n",
       "72369    0.000000        0.000875       0.001169       0  \n",
       "72370    0.000000        0.000000       0.000584       0  \n",
       "72371    0.000000        0.001749       0.002337       0  \n",
       "72372    0.000000        0.000000       0.000195       0  \n",
       "72373    0.000000        0.000000       0.000195       0  \n",
       "72374    0.000000        0.002186       0.000974       0  \n",
       "72375    0.000000        0.000000       0.000195       0  \n",
       "72376    0.000000        0.000000       0.000390       0  \n",
       "72377    0.000000        0.000000       0.000195       0  \n",
       "72378    0.000000        0.000000       0.000195       0  \n",
       "72379    0.000000        0.000000       0.000195       0  \n",
       "72380    0.000000        0.000000       0.000195       0  \n",
       "72381    0.005917        0.000437       0.001753       0  \n",
       "72382    0.002959        0.000437       0.001753       0  \n",
       "72383    0.000000        0.000000       0.000390       0  \n",
       "72384    0.000000        0.000000       0.000974       0  \n",
       "72385    0.002959        0.000000       0.000974       0  \n",
       "72386    0.000000        0.000000       0.000195       0  \n",
       "72387    0.000000        0.000000       0.001558       0  \n",
       "72388    0.000000        0.000437       0.000779       0  \n",
       "72389    0.000000        0.000000       0.000195       0  \n",
       "72390    0.002959        0.000875       0.001753       0  \n",
       "72391    0.000000        0.000000       0.000195       0  \n",
       "72392    0.000000        0.000000       0.000195       0  \n",
       "72393    0.000000        0.001312       0.000974       0  \n",
       "72394    0.000000        0.000000       0.000195       0  \n",
       "\n",
       "[72395 rows x 49 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23983</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23984</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23985</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23986</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23987</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23988</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23989</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23990</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23991</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23992</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23993</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23994</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24001</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24003</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24004</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24005</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24006</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24007</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24008</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24009</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24010</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24011</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24012</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24013 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Drop\n",
       "0         0\n",
       "1         1\n",
       "2         0\n",
       "3         0\n",
       "4         1\n",
       "5         0\n",
       "6         0\n",
       "7         0\n",
       "8         0\n",
       "9         1\n",
       "10        1\n",
       "11        1\n",
       "12        0\n",
       "13        1\n",
       "14        0\n",
       "15        0\n",
       "16        0\n",
       "17        1\n",
       "18        1\n",
       "19        0\n",
       "20        1\n",
       "21        0\n",
       "22        0\n",
       "23        0\n",
       "24        0\n",
       "25        1\n",
       "26        1\n",
       "27        0\n",
       "28        1\n",
       "29        0\n",
       "...     ...\n",
       "23983     1\n",
       "23984     1\n",
       "23985     1\n",
       "23986     1\n",
       "23987     1\n",
       "23988     1\n",
       "23989     1\n",
       "23990     1\n",
       "23991     1\n",
       "23992     1\n",
       "23993     0\n",
       "23994     1\n",
       "23995     1\n",
       "23996     1\n",
       "23997     1\n",
       "23998     1\n",
       "23999     1\n",
       "24000     1\n",
       "24001     1\n",
       "24002     1\n",
       "24003     1\n",
       "24004     1\n",
       "24005     1\n",
       "24006     1\n",
       "24007     1\n",
       "24008     1\n",
       "24009     1\n",
       "24010     1\n",
       "24011     1\n",
       "24012     1\n",
       "\n",
       "[24013 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    57366\n",
       "0    15029\n",
       "Name: Drop, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_count = y_train.Drop.value_counts()\n",
    "target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([X_train,y_train],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    57366\n",
      "0    57366\n",
      "Name: Drop, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_Drop_1, count_Drop_0 = train.Drop.value_counts()\n",
    "train_Drop_0 = train[train['Drop'] == 0]\n",
    "train_Drop_1 = train[train['Drop'] == 1]\n",
    "train_Drop_0_over = train_Drop_0.sample(count_Drop_1, replace=True)\n",
    "train_over = pd.concat([train_Drop_1, train_Drop_0_over], axis=0)\n",
    "print(train_over.Drop.value_counts())\n",
    "trs = train_over.sort_values(by=['enrollment_id'])\n",
    "tr = trs.reset_index(drop=True)\n",
    "y_train = tr['Drop']\n",
    "X_train = tr.drop(['Drop'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainlog = X_train.drop(['enrollment_id'],axis=1)\n",
    "y_trainlog = np.array(y_train)\n",
    "X_testlog = X_test.drop(['enrollment_id'],axis=1)\n",
    "y_testlog = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainlog = X_trainos\n",
    "y_trainlog = np.array(y_trainos)\n",
    "X_testlog = X_testos\n",
    "y_testlog = np.array(y_testos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:1092: RuntimeWarning: invalid value encountered in sqrt\n",
      "  bse_ = np.sqrt(np.diag(self.cov_params()))\n",
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>114732</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>114684</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    47</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 28 Nov 2018</td> <th>  Pseudo R-squ.:     </th>  <td>0.3551</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>23:31:52</td>     <th>  Log-Likelihood:    </th> <td> -51290.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>False</td>      <th>  LL-Null:           </th> <td> -79526.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                            <td>    1.0473</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1pvLqtotBsKv7QSOsLicJDQMHx3lui6d</th> <td>   -0.3011</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or</th> <td>    0.2311</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3cnZpv6ReApmCaZyaQwi2izDZxVRdC01</th> <td>   -0.2895</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6</th> <td>    0.1831</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2</th> <td>   -0.0038</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx</th> <td>    0.2915</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv</th> <td>   -0.3436</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz</th> <td>   -0.4468</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x</th> <td>   -0.1345</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9zpXzW9zCfU8KGBWkhlsGH8B8czISH4J</th> <td>    0.0827</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>A3fsA9Zfv1X2fVEQhTw51lKENdNrEqT3</th> <td>   -0.1152</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AXUJZGmZ0xaYSWazu8RQ1G5c76ECT1Kd</th> <td>    0.1161</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DABrJ6O4AotFwuAbfo1fuMj40VmMpPGX</th> <td>    0.1206</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DPnLzkJJqOOPRJfBxIHbQEERiYHu5ila</th> <td>   -0.1004</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Er0RFawC4sHagDmmQZcBGBrzamLQcblZ</th> <td>    0.2188</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>G8EPVSXsOYB5YQWZGiz1aVq5Pgr2GrQu</th> <td>   -0.2077</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>H2lDW05SyKnwntZ6Fora76aPAEswcMa5</th> <td>   -0.4729</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HbeAZjZFFQUe90oTP0RRO0PEtRAqU3kK</th> <td>    0.1835</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I7Go4XwWgpjRJM8EZGEnBpkfSmBNOlsO</th> <td>    0.1110</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>KHPw0gmg1Ad3V07TqRpyBzA8mRjj7mkt</th> <td>   -0.0747</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NmbZ3BmS8V4pMg6oxXHWpqqMZCE1jvYt</th> <td>   -0.0871</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RXDvfPUBYFlVdlueBFbLW0mhhAyGEqpt</th> <td>    0.1495</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SpATywNh6bZuzm8s1ceuBUnMUAeoAHHw</th> <td>   -0.5811</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAYxxh39I2LZnftBpL0LfF2NxzrCKpkx</th> <td>   -0.1486</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4tXq15GxHo2gaMpaJLZ3IGEkP949IbE</th> <td>   -0.1558</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WM572q68zD5VW8pcvVTc1RhhFUq3iRFN</th> <td>    0.0725</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Wm3dddHSynJ76EJV6hyLYKGGRL0JF3YK</th> <td>    0.3453</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X78EhlW2JxwO1I6S3U4yZVwkEQpKXLOj</th> <td>   -0.1868</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>a2E7NQC7nZB7WHEhKGhKnKvUWtsLAQzh</th> <td>    0.2356</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bWdj2GDclj5ofokWjzoa5jAwMkxCykd6</th> <td>   -0.0967</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fbPkOYLVPtPgIt0MxizjfFJov3JbHyAi</th> <td>    0.7437</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gvEwgd64UX4t3K7ftZwXiMkFuxFUAqQE</th> <td>    0.6008</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mTmmr5zd8l4wXhwiULwjSmSbi9ktcFmV</th> <td>    0.3025</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nSfGxfEtzw5G72fVbfaowxsV46Pg1xIc</th> <td>    0.2086</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>q6A6QG7qMpyNcznyT2XaIxnfNGkZRxXl</th> <td>    0.1926</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shM3Yy9vxHn2aqjSYfQXOcwGo0hWh3MI</th> <td>   -0.1212</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tXbz2ZYaRyb2ZsWUBPoYzAmisOhHQrYl</th> <td>    0.7151</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>xMd9DzNyUCTLRPVbwWVzf4vq06oqrTT1</th> <td>   -0.3426</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ</th> <td>    0.1523</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_access</th>                     <td>   43.1753</td> <td>    5.732</td> <td>    7.533</td> <td> 0.000</td> <td>   31.941</td> <td>   54.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_discussion</th>                 <td>  221.6382</td> <td>   12.281</td> <td>   18.047</td> <td> 0.000</td> <td>  197.568</td> <td>  245.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_navigate</th>                   <td>    6.1315</td> <td>    2.115</td> <td>    2.899</td> <td> 0.004</td> <td>    1.987</td> <td>   10.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_page_close</th>                 <td>   -1.1540</td> <td>    1.133</td> <td>   -1.019</td> <td> 0.308</td> <td>   -3.374</td> <td>    1.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_problem</th>                    <td>    8.7619</td> <td>    1.893</td> <td>    4.629</td> <td> 0.000</td> <td>    5.052</td> <td>   12.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_wiki</th>                       <td>   23.0275</td> <td>    1.519</td> <td>   15.155</td> <td> 0.000</td> <td>   20.049</td> <td>   26.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>source_browser</th>                   <td>  -22.5943</td> <td>    4.126</td> <td>   -5.476</td> <td> 0.000</td> <td>  -30.681</td> <td>  -14.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>source_server</th>                    <td> -268.6443</td> <td>   14.147</td> <td>  -18.990</td> <td> 0.000</td> <td> -296.371</td> <td> -240.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>skip10</th>                           <td>    0.8684</td> <td>    0.016</td> <td>   54.375</td> <td> 0.000</td> <td>    0.837</td> <td>    0.900</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:               114732\n",
       "Model:                          Logit   Df Residuals:                   114684\n",
       "Method:                           MLE   Df Model:                           47\n",
       "Date:                Wed, 28 Nov 2018   Pseudo R-squ.:                  0.3551\n",
       "Time:                        23:31:52   Log-Likelihood:                -51290.\n",
       "converged:                      False   LL-Null:                       -79526.\n",
       "                                        LLR p-value:                     0.000\n",
       "====================================================================================================\n",
       "                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------\n",
       "const                                1.0473        nan        nan        nan         nan         nan\n",
       "1pvLqtotBsKv7QSOsLicJDQMHx3lui6d    -0.3011        nan        nan        nan         nan         nan\n",
       "3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or     0.2311        nan        nan        nan         nan         nan\n",
       "3cnZpv6ReApmCaZyaQwi2izDZxVRdC01    -0.2895        nan        nan        nan         nan         nan\n",
       "5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6     0.1831        nan        nan        nan         nan         nan\n",
       "5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2    -0.0038        nan        nan        nan         nan         nan\n",
       "7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx     0.2915        nan        nan        nan         nan         nan\n",
       "81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv    -0.3436        nan        nan        nan         nan         nan\n",
       "9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz    -0.4468        nan        nan        nan         nan         nan\n",
       "9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x    -0.1345        nan        nan        nan         nan         nan\n",
       "9zpXzW9zCfU8KGBWkhlsGH8B8czISH4J     0.0827        nan        nan        nan         nan         nan\n",
       "A3fsA9Zfv1X2fVEQhTw51lKENdNrEqT3    -0.1152        nan        nan        nan         nan         nan\n",
       "AXUJZGmZ0xaYSWazu8RQ1G5c76ECT1Kd     0.1161        nan        nan        nan         nan         nan\n",
       "DABrJ6O4AotFwuAbfo1fuMj40VmMpPGX     0.1206        nan        nan        nan         nan         nan\n",
       "DPnLzkJJqOOPRJfBxIHbQEERiYHu5ila    -0.1004        nan        nan        nan         nan         nan\n",
       "Er0RFawC4sHagDmmQZcBGBrzamLQcblZ     0.2188        nan        nan        nan         nan         nan\n",
       "G8EPVSXsOYB5YQWZGiz1aVq5Pgr2GrQu    -0.2077        nan        nan        nan         nan         nan\n",
       "H2lDW05SyKnwntZ6Fora76aPAEswcMa5    -0.4729        nan        nan        nan         nan         nan\n",
       "HbeAZjZFFQUe90oTP0RRO0PEtRAqU3kK     0.1835        nan        nan        nan         nan         nan\n",
       "I7Go4XwWgpjRJM8EZGEnBpkfSmBNOlsO     0.1110        nan        nan        nan         nan         nan\n",
       "KHPw0gmg1Ad3V07TqRpyBzA8mRjj7mkt    -0.0747        nan        nan        nan         nan         nan\n",
       "NmbZ3BmS8V4pMg6oxXHWpqqMZCE1jvYt    -0.0871        nan        nan        nan         nan         nan\n",
       "RXDvfPUBYFlVdlueBFbLW0mhhAyGEqpt     0.1495        nan        nan        nan         nan         nan\n",
       "SpATywNh6bZuzm8s1ceuBUnMUAeoAHHw    -0.5811        nan        nan        nan         nan         nan\n",
       "TAYxxh39I2LZnftBpL0LfF2NxzrCKpkx    -0.1486        nan        nan        nan         nan         nan\n",
       "V4tXq15GxHo2gaMpaJLZ3IGEkP949IbE    -0.1558        nan        nan        nan         nan         nan\n",
       "WM572q68zD5VW8pcvVTc1RhhFUq3iRFN     0.0725        nan        nan        nan         nan         nan\n",
       "Wm3dddHSynJ76EJV6hyLYKGGRL0JF3YK     0.3453        nan        nan        nan         nan         nan\n",
       "X78EhlW2JxwO1I6S3U4yZVwkEQpKXLOj    -0.1868        nan        nan        nan         nan         nan\n",
       "a2E7NQC7nZB7WHEhKGhKnKvUWtsLAQzh     0.2356        nan        nan        nan         nan         nan\n",
       "bWdj2GDclj5ofokWjzoa5jAwMkxCykd6    -0.0967        nan        nan        nan         nan         nan\n",
       "fbPkOYLVPtPgIt0MxizjfFJov3JbHyAi     0.7437        nan        nan        nan         nan         nan\n",
       "gvEwgd64UX4t3K7ftZwXiMkFuxFUAqQE     0.6008        nan        nan        nan         nan         nan\n",
       "mTmmr5zd8l4wXhwiULwjSmSbi9ktcFmV     0.3025        nan        nan        nan         nan         nan\n",
       "nSfGxfEtzw5G72fVbfaowxsV46Pg1xIc     0.2086        nan        nan        nan         nan         nan\n",
       "q6A6QG7qMpyNcznyT2XaIxnfNGkZRxXl     0.1926        nan        nan        nan         nan         nan\n",
       "shM3Yy9vxHn2aqjSYfQXOcwGo0hWh3MI    -0.1212        nan        nan        nan         nan         nan\n",
       "tXbz2ZYaRyb2ZsWUBPoYzAmisOhHQrYl     0.7151        nan        nan        nan         nan         nan\n",
       "xMd9DzNyUCTLRPVbwWVzf4vq06oqrTT1    -0.3426        nan        nan        nan         nan         nan\n",
       "ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ     0.1523        nan        nan        nan         nan         nan\n",
       "event_access                        43.1753      5.732      7.533      0.000      31.941      54.409\n",
       "event_discussion                   221.6382     12.281     18.047      0.000     197.568     245.709\n",
       "event_navigate                       6.1315      2.115      2.899      0.004       1.987      10.276\n",
       "event_page_close                    -1.1540      1.133     -1.019      0.308      -3.374       1.066\n",
       "event_problem                        8.7619      1.893      4.629      0.000       5.052      12.472\n",
       "event_wiki                          23.0275      1.519     15.155      0.000      20.049      26.006\n",
       "source_browser                     -22.5943      4.126     -5.476      0.000     -30.681     -14.508\n",
       "source_server                     -268.6443     14.147    -18.990      0.000    -296.371    -240.918\n",
       "skip10                               0.8684      0.016     54.375      0.000       0.837       0.900\n",
       "====================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Instantiate The Logistic Regression\n",
    "sm_model = sm.Logit(y_trainlog, sm.add_constant(X_trainlog)).fit(disp=0)\n",
    "sm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1pvLqtotBsKv7QSOsLicJDQMHx3lui6d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1</td>\n",
       "      <td>3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1</td>\n",
       "      <td>3cnZpv6ReApmCaZyaQwi2izDZxVRdC01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.1</td>\n",
       "      <td>5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.3</td>\n",
       "      <td>81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.2</td>\n",
       "      <td>9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9zpXzW9zCfU8KGBWkhlsGH8B8czISH4J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.1</td>\n",
       "      <td>A3fsA9Zfv1X2fVEQhTw51lKENdNrEqT3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2</td>\n",
       "      <td>AXUJZGmZ0xaYSWazu8RQ1G5c76ECT1Kd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>DABrJ6O4AotFwuAbfo1fuMj40VmMpPGX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>DPnLzkJJqOOPRJfBxIHbQEERiYHu5ila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.1</td>\n",
       "      <td>Er0RFawC4sHagDmmQZcBGBrzamLQcblZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>G8EPVSXsOYB5YQWZGiz1aVq5Pgr2GrQu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.3</td>\n",
       "      <td>H2lDW05SyKnwntZ6Fora76aPAEswcMa5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.1</td>\n",
       "      <td>HbeAZjZFFQUe90oTP0RRO0PEtRAqU3kK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.5</td>\n",
       "      <td>I7Go4XwWgpjRJM8EZGEnBpkfSmBNOlsO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>KHPw0gmg1Ad3V07TqRpyBzA8mRjj7mkt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.1</td>\n",
       "      <td>NmbZ3BmS8V4pMg6oxXHWpqqMZCE1jvYt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RXDvfPUBYFlVdlueBFbLW0mhhAyGEqpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.2</td>\n",
       "      <td>SpATywNh6bZuzm8s1ceuBUnMUAeoAHHw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>TAYxxh39I2LZnftBpL0LfF2NxzrCKpkx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.1</td>\n",
       "      <td>V4tXq15GxHo2gaMpaJLZ3IGEkP949IbE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>WM572q68zD5VW8pcvVTc1RhhFUq3iRFN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Wm3dddHSynJ76EJV6hyLYKGGRL0JF3YK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>X78EhlW2JxwO1I6S3U4yZVwkEQpKXLOj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.1</td>\n",
       "      <td>a2E7NQC7nZB7WHEhKGhKnKvUWtsLAQzh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>bWdj2GDclj5ofokWjzoa5jAwMkxCykd6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fbPkOYLVPtPgIt0MxizjfFJov3JbHyAi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>gvEwgd64UX4t3K7ftZwXiMkFuxFUAqQE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>mTmmr5zd8l4wXhwiULwjSmSbi9ktcFmV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.1</td>\n",
       "      <td>nSfGxfEtzw5G72fVbfaowxsV46Pg1xIc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>q6A6QG7qMpyNcznyT2XaIxnfNGkZRxXl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.2</td>\n",
       "      <td>shM3Yy9vxHn2aqjSYfQXOcwGo0hWh3MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>tXbz2ZYaRyb2ZsWUBPoYzAmisOhHQrYl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.1</td>\n",
       "      <td>xMd9DzNyUCTLRPVbwWVzf4vq06oqrTT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.1</td>\n",
       "      <td>ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>147.0</td>\n",
       "      <td>event_access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>67.1</td>\n",
       "      <td>event_discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>14.5</td>\n",
       "      <td>event_navigate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>29.0</td>\n",
       "      <td>event_page_close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>37.2</td>\n",
       "      <td>event_problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.9</td>\n",
       "      <td>event_wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>159.2</td>\n",
       "      <td>source_browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>318.4</td>\n",
       "      <td>source_server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.2</td>\n",
       "      <td>skip10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    VIF Factor                          features\n",
       "0          1.1  1pvLqtotBsKv7QSOsLicJDQMHx3lui6d\n",
       "1          1.1  3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or\n",
       "2          1.1  3cnZpv6ReApmCaZyaQwi2izDZxVRdC01\n",
       "3          1.1  5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6\n",
       "4          1.0  5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2\n",
       "5          1.0  7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx\n",
       "6          1.3  81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv\n",
       "7          1.2  9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz\n",
       "8          1.0  9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x\n",
       "9          1.0  9zpXzW9zCfU8KGBWkhlsGH8B8czISH4J\n",
       "10         1.1  A3fsA9Zfv1X2fVEQhTw51lKENdNrEqT3\n",
       "11         1.2  AXUJZGmZ0xaYSWazu8RQ1G5c76ECT1Kd\n",
       "12         1.0  DABrJ6O4AotFwuAbfo1fuMj40VmMpPGX\n",
       "13         1.4  DPnLzkJJqOOPRJfBxIHbQEERiYHu5ila\n",
       "14         1.1  Er0RFawC4sHagDmmQZcBGBrzamLQcblZ\n",
       "15         1.0  G8EPVSXsOYB5YQWZGiz1aVq5Pgr2GrQu\n",
       "16         1.3  H2lDW05SyKnwntZ6Fora76aPAEswcMa5\n",
       "17         1.1  HbeAZjZFFQUe90oTP0RRO0PEtRAqU3kK\n",
       "18         1.5  I7Go4XwWgpjRJM8EZGEnBpkfSmBNOlsO\n",
       "19         1.0  KHPw0gmg1Ad3V07TqRpyBzA8mRjj7mkt\n",
       "20         1.1  NmbZ3BmS8V4pMg6oxXHWpqqMZCE1jvYt\n",
       "21         1.0  RXDvfPUBYFlVdlueBFbLW0mhhAyGEqpt\n",
       "22         1.2  SpATywNh6bZuzm8s1ceuBUnMUAeoAHHw\n",
       "23         1.0  TAYxxh39I2LZnftBpL0LfF2NxzrCKpkx\n",
       "24         1.1  V4tXq15GxHo2gaMpaJLZ3IGEkP949IbE\n",
       "25         1.0  WM572q68zD5VW8pcvVTc1RhhFUq3iRFN\n",
       "26         1.0  Wm3dddHSynJ76EJV6hyLYKGGRL0JF3YK\n",
       "27         1.0  X78EhlW2JxwO1I6S3U4yZVwkEQpKXLOj\n",
       "28         1.1  a2E7NQC7nZB7WHEhKGhKnKvUWtsLAQzh\n",
       "29         1.0  bWdj2GDclj5ofokWjzoa5jAwMkxCykd6\n",
       "30         1.0  fbPkOYLVPtPgIt0MxizjfFJov3JbHyAi\n",
       "31         1.0  gvEwgd64UX4t3K7ftZwXiMkFuxFUAqQE\n",
       "32         1.0  mTmmr5zd8l4wXhwiULwjSmSbi9ktcFmV\n",
       "33         1.1  nSfGxfEtzw5G72fVbfaowxsV46Pg1xIc\n",
       "34         1.0  q6A6QG7qMpyNcznyT2XaIxnfNGkZRxXl\n",
       "35         1.2  shM3Yy9vxHn2aqjSYfQXOcwGo0hWh3MI\n",
       "36         1.0  tXbz2ZYaRyb2ZsWUBPoYzAmisOhHQrYl\n",
       "37         1.1  xMd9DzNyUCTLRPVbwWVzf4vq06oqrTT1\n",
       "38         1.1  ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ\n",
       "39       147.0                      event_access\n",
       "40        67.1                  event_discussion\n",
       "41        14.5                    event_navigate\n",
       "42        29.0                  event_page_close\n",
       "43        37.2                     event_problem\n",
       "44         1.9                        event_wiki\n",
       "45       159.2                    source_browser\n",
       "46       318.4                     source_server\n",
       "47         1.2                            skip10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each Xi, calculate VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X_trainlog.values, i) for i in range(X_trainlog.shape[1])]\n",
    "vif[\"features\"] = X_trainlog.columns\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vifcl(df):\n",
    "    a = 0\n",
    "    i = 0\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    vif[\"features\"] = df.columns\n",
    "    t = vif[\"VIF Factor\"].max()\n",
    "    for each in vif[\"VIF Factor\"]:\n",
    "        if each == t and each >= 10:\n",
    "            a = i\n",
    "        i += 1\n",
    "    vax = vif[\"VIF Factor\"].loc[a]\n",
    "    print(vif.loc[a],\"\\n>10\")\n",
    "    return a,vax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Factor          318.433\n",
      "features      source_server\n",
      "Name: 46, dtype: object \n",
      ">10\n",
      "318.43255055842076\n",
      "dropped source_server\n",
      "VIF Factor           30.4424\n",
      "features      source_browser\n",
      "Name: 45, dtype: object \n",
      ">10\n",
      "30.442422255068035\n",
      "dropped source_browser\n",
      "VIF Factor                             1.07837\n",
      "features      1pvLqtotBsKv7QSOsLicJDQMHx3lui6d\n",
      "Name: 0, dtype: object \n",
      ">10\n",
      "1.078371537712656\n"
     ]
    }
   ],
   "source": [
    "#drop VIF>10\n",
    "dropcindex=[]\n",
    "while True:\n",
    "    vmax = 0\n",
    "    dropt = False\n",
    "    k,s = vifcl(X_trainlog)\n",
    "    print(s)\n",
    "    if s >= 10:\n",
    "        dropt = True\n",
    "    if dropt == True:\n",
    "        print(\"dropped\",X_trainlog.columns.get_values()[k])\n",
    "        X_trainlog = X_trainlog.drop([X_trainlog.columns.get_values()[k]],axis=1)\n",
    "        vif = vif.drop([k])\n",
    "        X_trainlog = X_trainlog.reset_index(drop=True)\n",
    "        vif = vif.reset_index(drop=True)\n",
    "        dropcindex.append(k)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>114732</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>114686</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    45</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 28 Nov 2018</td> <th>  Pseudo R-squ.:     </th>  <td>0.3491</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>23:35:25</td>     <th>  Log-Likelihood:    </th> <td> -51760.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>False</td>      <th>  LL-Null:           </th> <td> -79526.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                            <td>    1.0376</td> <td> 2.92e+05</td> <td> 3.56e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1pvLqtotBsKv7QSOsLicJDQMHx3lui6d</th> <td>   -0.0690</td> <td> 2.92e+05</td> <td>-2.36e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or</th> <td>    0.3321</td> <td> 2.92e+05</td> <td> 1.14e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3cnZpv6ReApmCaZyaQwi2izDZxVRdC01</th> <td>   -0.3629</td> <td> 2.92e+05</td> <td>-1.24e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6</th> <td>    0.1792</td> <td> 2.92e+05</td> <td> 6.14e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2</th> <td>    0.0390</td> <td> 2.92e+05</td> <td> 1.34e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx</th> <td>    0.3687</td> <td> 2.92e+05</td> <td> 1.26e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv</th> <td>   -0.3262</td> <td> 2.92e+05</td> <td>-1.12e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz</th> <td>   -0.3577</td> <td> 2.92e+05</td> <td>-1.23e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x</th> <td>   -0.1417</td> <td> 2.92e+05</td> <td>-4.86e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9zpXzW9zCfU8KGBWkhlsGH8B8czISH4J</th> <td>    0.1027</td> <td> 2.92e+05</td> <td> 3.52e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>A3fsA9Zfv1X2fVEQhTw51lKENdNrEqT3</th> <td>   -0.2492</td> <td> 2.92e+05</td> <td>-8.54e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AXUJZGmZ0xaYSWazu8RQ1G5c76ECT1Kd</th> <td>   -0.0175</td> <td> 2.92e+05</td> <td>-6.01e-08</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DABrJ6O4AotFwuAbfo1fuMj40VmMpPGX</th> <td>    0.0465</td> <td> 2.92e+05</td> <td>  1.6e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DPnLzkJJqOOPRJfBxIHbQEERiYHu5ila</th> <td>   -0.2338</td> <td> 2.92e+05</td> <td>-8.01e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Er0RFawC4sHagDmmQZcBGBrzamLQcblZ</th> <td>    0.3530</td> <td> 2.92e+05</td> <td> 1.21e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>G8EPVSXsOYB5YQWZGiz1aVq5Pgr2GrQu</th> <td>   -0.3090</td> <td> 2.92e+05</td> <td>-1.06e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>H2lDW05SyKnwntZ6Fora76aPAEswcMa5</th> <td>   -0.6103</td> <td> 2.92e+05</td> <td>-2.09e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HbeAZjZFFQUe90oTP0RRO0PEtRAqU3kK</th> <td>    0.1994</td> <td> 2.92e+05</td> <td> 6.83e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I7Go4XwWgpjRJM8EZGEnBpkfSmBNOlsO</th> <td>    0.0193</td> <td> 2.92e+05</td> <td> 6.62e-08</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>KHPw0gmg1Ad3V07TqRpyBzA8mRjj7mkt</th> <td>   -0.0748</td> <td> 2.92e+05</td> <td>-2.56e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NmbZ3BmS8V4pMg6oxXHWpqqMZCE1jvYt</th> <td>   -0.0804</td> <td> 2.92e+05</td> <td>-2.75e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RXDvfPUBYFlVdlueBFbLW0mhhAyGEqpt</th> <td>    0.0733</td> <td> 2.92e+05</td> <td> 2.51e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SpATywNh6bZuzm8s1ceuBUnMUAeoAHHw</th> <td>   -0.6046</td> <td> 2.92e+05</td> <td>-2.07e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAYxxh39I2LZnftBpL0LfF2NxzrCKpkx</th> <td>   -0.0469</td> <td> 2.92e+05</td> <td>-1.61e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4tXq15GxHo2gaMpaJLZ3IGEkP949IbE</th> <td>   -0.0530</td> <td> 2.92e+05</td> <td>-1.82e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WM572q68zD5VW8pcvVTc1RhhFUq3iRFN</th> <td>    0.1518</td> <td> 2.92e+05</td> <td>  5.2e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Wm3dddHSynJ76EJV6hyLYKGGRL0JF3YK</th> <td>    0.4433</td> <td> 2.92e+05</td> <td> 1.52e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X78EhlW2JxwO1I6S3U4yZVwkEQpKXLOj</th> <td>   -0.3151</td> <td> 2.92e+05</td> <td>-1.08e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>a2E7NQC7nZB7WHEhKGhKnKvUWtsLAQzh</th> <td>    0.1318</td> <td> 2.92e+05</td> <td> 4.51e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bWdj2GDclj5ofokWjzoa5jAwMkxCykd6</th> <td>   -0.1472</td> <td> 2.92e+05</td> <td>-5.04e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fbPkOYLVPtPgIt0MxizjfFJov3JbHyAi</th> <td>    0.7509</td> <td> 2.92e+05</td> <td> 2.57e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gvEwgd64UX4t3K7ftZwXiMkFuxFUAqQE</th> <td>    0.5374</td> <td> 2.92e+05</td> <td> 1.84e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mTmmr5zd8l4wXhwiULwjSmSbi9ktcFmV</th> <td>    0.3755</td> <td> 2.92e+05</td> <td> 1.29e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nSfGxfEtzw5G72fVbfaowxsV46Pg1xIc</th> <td>    0.3151</td> <td> 2.92e+05</td> <td> 1.08e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>q6A6QG7qMpyNcznyT2XaIxnfNGkZRxXl</th> <td>    0.2636</td> <td> 2.92e+05</td> <td> 9.03e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shM3Yy9vxHn2aqjSYfQXOcwGo0hWh3MI</th> <td>   -0.0029</td> <td> 2.92e+05</td> <td>-9.78e-09</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tXbz2ZYaRyb2ZsWUBPoYzAmisOhHQrYl</th> <td>    0.7880</td> <td> 2.92e+05</td> <td>  2.7e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>xMd9DzNyUCTLRPVbwWVzf4vq06oqrTT1</th> <td>   -0.4712</td> <td> 2.92e+05</td> <td>-1.61e-06</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ</th> <td>    0.0405</td> <td> 2.92e+05</td> <td> 1.39e-07</td> <td> 1.000</td> <td>-5.72e+05</td> <td> 5.72e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_access</th>                     <td>  -39.0012</td> <td>    1.279</td> <td>  -30.491</td> <td> 0.000</td> <td>  -41.508</td> <td>  -36.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_discussion</th>                 <td>   -1.3358</td> <td>    3.458</td> <td>   -0.386</td> <td> 0.699</td> <td>   -8.113</td> <td>    5.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_navigate</th>                   <td>  -33.5053</td> <td>    1.061</td> <td>  -31.594</td> <td> 0.000</td> <td>  -35.584</td> <td>  -31.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_page_close</th>                 <td>  -10.8638</td> <td>    0.414</td> <td>  -26.259</td> <td> 0.000</td> <td>  -11.675</td> <td>  -10.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_problem</th>                    <td>   -0.9954</td> <td>    0.510</td> <td>   -1.952</td> <td> 0.051</td> <td>   -1.995</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>event_wiki</th>                       <td>    5.5039</td> <td>    1.154</td> <td>    4.768</td> <td> 0.000</td> <td>    3.241</td> <td>    7.767</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>skip10</th>                           <td>    0.8919</td> <td>    0.016</td> <td>   56.212</td> <td> 0.000</td> <td>    0.861</td> <td>    0.923</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:               114732\n",
       "Model:                          Logit   Df Residuals:                   114686\n",
       "Method:                           MLE   Df Model:                           45\n",
       "Date:                Wed, 28 Nov 2018   Pseudo R-squ.:                  0.3491\n",
       "Time:                        23:35:25   Log-Likelihood:                -51760.\n",
       "converged:                      False   LL-Null:                       -79526.\n",
       "                                        LLR p-value:                     0.000\n",
       "====================================================================================================\n",
       "                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------\n",
       "const                                1.0376   2.92e+05   3.56e-06      1.000   -5.72e+05    5.72e+05\n",
       "1pvLqtotBsKv7QSOsLicJDQMHx3lui6d    -0.0690   2.92e+05  -2.36e-07      1.000   -5.72e+05    5.72e+05\n",
       "3VkHkmOtom3jM2wCu94xgzzu1d6Dn7or     0.3321   2.92e+05   1.14e-06      1.000   -5.72e+05    5.72e+05\n",
       "3cnZpv6ReApmCaZyaQwi2izDZxVRdC01    -0.3629   2.92e+05  -1.24e-06      1.000   -5.72e+05    5.72e+05\n",
       "5Gyp41oLVo7Gg7vF4vpmggWP5MU70QO6     0.1792   2.92e+05   6.14e-07      1.000   -5.72e+05    5.72e+05\n",
       "5X6FeZozNMgE2VRi3MJYjkkFK8SETtu2     0.0390   2.92e+05   1.34e-07      1.000   -5.72e+05    5.72e+05\n",
       "7GRhBDsirIGkRZBtSMEzNTyDr2JQm4xx     0.3687   2.92e+05   1.26e-06      1.000   -5.72e+05    5.72e+05\n",
       "81UZtt1JJwBFYMj5u38WNKCSVA4IJSDv    -0.3262   2.92e+05  -1.12e-06      1.000   -5.72e+05    5.72e+05\n",
       "9Bd26pfDLvkPINwLnpaGcf0LrLUvY1Mz    -0.3577   2.92e+05  -1.23e-06      1.000   -5.72e+05    5.72e+05\n",
       "9Mq1P5hrrLw6Bh9X4W4ZjisQJDdxjz9x    -0.1417   2.92e+05  -4.86e-07      1.000   -5.72e+05    5.72e+05\n",
       "9zpXzW9zCfU8KGBWkhlsGH8B8czISH4J     0.1027   2.92e+05   3.52e-07      1.000   -5.72e+05    5.72e+05\n",
       "A3fsA9Zfv1X2fVEQhTw51lKENdNrEqT3    -0.2492   2.92e+05  -8.54e-07      1.000   -5.72e+05    5.72e+05\n",
       "AXUJZGmZ0xaYSWazu8RQ1G5c76ECT1Kd    -0.0175   2.92e+05  -6.01e-08      1.000   -5.72e+05    5.72e+05\n",
       "DABrJ6O4AotFwuAbfo1fuMj40VmMpPGX     0.0465   2.92e+05    1.6e-07      1.000   -5.72e+05    5.72e+05\n",
       "DPnLzkJJqOOPRJfBxIHbQEERiYHu5ila    -0.2338   2.92e+05  -8.01e-07      1.000   -5.72e+05    5.72e+05\n",
       "Er0RFawC4sHagDmmQZcBGBrzamLQcblZ     0.3530   2.92e+05   1.21e-06      1.000   -5.72e+05    5.72e+05\n",
       "G8EPVSXsOYB5YQWZGiz1aVq5Pgr2GrQu    -0.3090   2.92e+05  -1.06e-06      1.000   -5.72e+05    5.72e+05\n",
       "H2lDW05SyKnwntZ6Fora76aPAEswcMa5    -0.6103   2.92e+05  -2.09e-06      1.000   -5.72e+05    5.72e+05\n",
       "HbeAZjZFFQUe90oTP0RRO0PEtRAqU3kK     0.1994   2.92e+05   6.83e-07      1.000   -5.72e+05    5.72e+05\n",
       "I7Go4XwWgpjRJM8EZGEnBpkfSmBNOlsO     0.0193   2.92e+05   6.62e-08      1.000   -5.72e+05    5.72e+05\n",
       "KHPw0gmg1Ad3V07TqRpyBzA8mRjj7mkt    -0.0748   2.92e+05  -2.56e-07      1.000   -5.72e+05    5.72e+05\n",
       "NmbZ3BmS8V4pMg6oxXHWpqqMZCE1jvYt    -0.0804   2.92e+05  -2.75e-07      1.000   -5.72e+05    5.72e+05\n",
       "RXDvfPUBYFlVdlueBFbLW0mhhAyGEqpt     0.0733   2.92e+05   2.51e-07      1.000   -5.72e+05    5.72e+05\n",
       "SpATywNh6bZuzm8s1ceuBUnMUAeoAHHw    -0.6046   2.92e+05  -2.07e-06      1.000   -5.72e+05    5.72e+05\n",
       "TAYxxh39I2LZnftBpL0LfF2NxzrCKpkx    -0.0469   2.92e+05  -1.61e-07      1.000   -5.72e+05    5.72e+05\n",
       "V4tXq15GxHo2gaMpaJLZ3IGEkP949IbE    -0.0530   2.92e+05  -1.82e-07      1.000   -5.72e+05    5.72e+05\n",
       "WM572q68zD5VW8pcvVTc1RhhFUq3iRFN     0.1518   2.92e+05    5.2e-07      1.000   -5.72e+05    5.72e+05\n",
       "Wm3dddHSynJ76EJV6hyLYKGGRL0JF3YK     0.4433   2.92e+05   1.52e-06      1.000   -5.72e+05    5.72e+05\n",
       "X78EhlW2JxwO1I6S3U4yZVwkEQpKXLOj    -0.3151   2.92e+05  -1.08e-06      1.000   -5.72e+05    5.72e+05\n",
       "a2E7NQC7nZB7WHEhKGhKnKvUWtsLAQzh     0.1318   2.92e+05   4.51e-07      1.000   -5.72e+05    5.72e+05\n",
       "bWdj2GDclj5ofokWjzoa5jAwMkxCykd6    -0.1472   2.92e+05  -5.04e-07      1.000   -5.72e+05    5.72e+05\n",
       "fbPkOYLVPtPgIt0MxizjfFJov3JbHyAi     0.7509   2.92e+05   2.57e-06      1.000   -5.72e+05    5.72e+05\n",
       "gvEwgd64UX4t3K7ftZwXiMkFuxFUAqQE     0.5374   2.92e+05   1.84e-06      1.000   -5.72e+05    5.72e+05\n",
       "mTmmr5zd8l4wXhwiULwjSmSbi9ktcFmV     0.3755   2.92e+05   1.29e-06      1.000   -5.72e+05    5.72e+05\n",
       "nSfGxfEtzw5G72fVbfaowxsV46Pg1xIc     0.3151   2.92e+05   1.08e-06      1.000   -5.72e+05    5.72e+05\n",
       "q6A6QG7qMpyNcznyT2XaIxnfNGkZRxXl     0.2636   2.92e+05   9.03e-07      1.000   -5.72e+05    5.72e+05\n",
       "shM3Yy9vxHn2aqjSYfQXOcwGo0hWh3MI    -0.0029   2.92e+05  -9.78e-09      1.000   -5.72e+05    5.72e+05\n",
       "tXbz2ZYaRyb2ZsWUBPoYzAmisOhHQrYl     0.7880   2.92e+05    2.7e-06      1.000   -5.72e+05    5.72e+05\n",
       "xMd9DzNyUCTLRPVbwWVzf4vq06oqrTT1    -0.4712   2.92e+05  -1.61e-06      1.000   -5.72e+05    5.72e+05\n",
       "ykoe1cCWK134BJmfbNoPEenJOIWdtQOZ     0.0405   2.92e+05   1.39e-07      1.000   -5.72e+05    5.72e+05\n",
       "event_access                       -39.0012      1.279    -30.491      0.000     -41.508     -36.494\n",
       "event_discussion                    -1.3358      3.458     -0.386      0.699      -8.113       5.441\n",
       "event_navigate                     -33.5053      1.061    -31.594      0.000     -35.584     -31.427\n",
       "event_page_close                   -10.8638      0.414    -26.259      0.000     -11.675     -10.053\n",
       "event_problem                       -0.9954      0.510     -1.952      0.051      -1.995       0.004\n",
       "event_wiki                           5.5039      1.154      4.768      0.000       3.241       7.767\n",
       "skip10                               0.8919      0.016     56.212      0.000       0.861       0.923\n",
       "====================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Instantiate The Logistic Regression\n",
    "sm_model = sm.Logit(y_trainlog, sm.add_constant(X_trainlog)).fit(disp=0)\n",
    "sm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C = 0.01: 0.8326818150424753\n",
      "Accuracy for C = 0.1: 0.8539954416741488\n",
      "Accuracy for C = 1: 0.8596588162165896\n",
      "Accuracy for C = 10.0: 0.8607500517991574\n",
      "Accuracy for C = 100.0: 0.8612058843842807\n",
      "Accuracy for C = 1000.0: 0.8612058843842807\n",
      "Accuracy for C = 10000.0: 0.8612196974929208\n",
      "Accuracy for C = 100000.0: 0.8612196974929208\n",
      "Accuracy for C = 1000000.0: 0.8612058843842807\n",
      "Accuracy for C = 10000000.0: 0.8612058843842807\n",
      "Accuracy for C = 100000000.0: 0.8612196974929208\n",
      "Accuracy for C = 1000000000.0: 0.861247323710201\n",
      "Accuracy for C = 10000000000.0: 0.8612335106015608\n",
      "Accuracy for C = 100000000000.0: 0.8611920712756406\n",
      "Accuracy for C = 1000000000000.0: 0.8612058843842807\n",
      "0.861247323710201\n"
     ]
    }
   ],
   "source": [
    "#Tuning C\n",
    "Ce=[0.01,0.1,1,1e1,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9,1e10,1e11,1e12]\n",
    "ali = []\n",
    "for eac in Ce:\n",
    "    model = LogisticRegression(C = eac)\n",
    "    model = model.fit (X_trainlog,y_trainlog.ravel())\n",
    "    probability = model.predict_proba(X_trainlog)\n",
    "    predicted = model.predict(X_trainlog)\n",
    "    print ('Accuracy for C = {}:'.format(eac),model.score(X_trainlog,y_trainlog))\n",
    "    ali.append(model.score(X_trainlog,y_trainlog))\n",
    "print(max(ali))\n",
    "#C=1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for tol = 10: 0.2075972097520547\n",
      "Accuracy for tol = 1: 0.7924027902479454\n",
      "Accuracy for tol = 0.1: 0.8495614338006768\n",
      "Accuracy for tol = 0.01: 0.8606809862559569\n",
      "Accuracy for tol = 0.001: 0.8612611368188411\n",
      "Accuracy for tol = 0.0001: 0.861247323710201\n",
      "Accuracy for tol = 1e-05: 0.8612335106015608\n",
      "Accuracy for tol = 1e-06: 0.8612335106015608\n",
      "Accuracy for tol = 1e-07: 0.8612335106015608\n",
      "Accuracy for tol = 1e-08: 0.8612335106015608\n",
      "Accuracy for tol = 1e-09: 0.8612335106015608\n",
      "Accuracy for tol = 1e-10: 0.8612335106015608\n",
      "Accuracy for tol = 1e-11: 0.8612335106015608\n",
      "Accuracy for tol = 1e-12: 0.8612335106015608\n",
      "0.8612611368188411\n"
     ]
    }
   ],
   "source": [
    "#Tuning tol\n",
    "toor=[10,1,0.1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10,1e-11,1e-12]\n",
    "ali = []\n",
    "for tt in toor:\n",
    "    model = LogisticRegression(C = 1000000000, tol = tt)\n",
    "    model = model.fit (X_trainlog,y_trainlog.ravel())\n",
    "    probability = model.predict_proba(X_trainlog)\n",
    "    predicted = model.predict(X_trainlog)\n",
    "    print ('Accuracy for tol = {}:'.format(tt),model.score(X_trainlog,y_trainlog))\n",
    "    ali.append(model.score(X_trainlog,y_trainlog))\n",
    "print(max(ali))\n",
    "#tol=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for solver = newton-cg: 0.8612335106015608\n",
      "Accuracy for solver = lbfgs: 0.8607914911250777\n",
      "Accuracy for solver = liblinear: 0.8612611368188411\n",
      "Accuracy for solver = sag: 0.8612335106015608\n",
      "Accuracy for solver = saga: 0.8612058843842807\n",
      "0.8612611368188411\n"
     ]
    }
   ],
   "source": [
    "#Tuning tol\n",
    "ss=['newton-cg','lbfgs','liblinear','sag','saga']\n",
    "ali = []\n",
    "for ese in ss:\n",
    "    model = LogisticRegression(C = 1000000000, tol = 0.001,solver=ese)\n",
    "    model = model.fit (X_trainlog,y_trainlog.ravel())\n",
    "    probability = model.predict_proba(X_trainlog)\n",
    "    predicted = model.predict(X_trainlog)\n",
    "    print ('Accuracy for solver = {}:'.format(ese),model.score(X_trainlog,y_trainlog))\n",
    "    ali.append(model.score(X_trainlog,y_trainlog))\n",
    "print(max(ali))\n",
    "#tol=liblinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the cleansed data again\n",
    "model = LogisticRegression(C = 1000000000,tol=0.001)\n",
    "model = model.fit (X_trainlog,y_trainlog.ravel())\n",
    "probability = model.predict_proba(X_trainlog)\n",
    "predicted = model.predict(X_trainlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41596 15770]\n",
      " [ 6676 50690]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.73      0.79     57366\n",
      "          1       0.76      0.88      0.82     57366\n",
      "\n",
      "avg / total       0.81      0.80      0.80    114732\n",
      "\n",
      "Accuracy: 0.8043614684656417\n"
     ]
    }
   ],
   "source": [
    "## Evaluate The Model Confusion Matrix\n",
    "print (metrics.confusion_matrix(y_trainlog, predicted))\n",
    "## Classification Report\n",
    "print (metrics.classification_report(y_trainlog, predicted))\n",
    "## Model Accuracy\n",
    "print ('Accuracy:',model.score(X_trainlog,y_trainlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Prediction Performance â€“ ROC curve & AUC\n",
    "y_predict_probabilities = probability[:,1]\n",
    "fpr, tpr, _ = roc_curve(y_trainlog, y_predict_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testlog = X_testlog.drop(['source_server','source_browser'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprobability = model.predict_proba(X_testlog)\n",
    "tpredicted = model.predict(X_testlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3522  1380]\n",
      " [ 2863 16248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.72      0.62      4902\n",
      "          1       0.92      0.85      0.88     19111\n",
      "\n",
      "avg / total       0.85      0.82      0.83     24013\n",
      "\n",
      "Accuracy: 0.8233040436430267\n"
     ]
    }
   ],
   "source": [
    "## Evaluate The Model Confusion Matrix\n",
    "print (metrics.confusion_matrix(y_testlog, tpredicted))\n",
    "## Classification Report\n",
    "print (metrics.classification_report(y_testlog, tpredicted))\n",
    "## Model Accuracy\n",
    "print ('Accuracy:',model.score(X_testlog,y_testlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Prediction Performance â€“ ROC curve & AUC\n",
    "y_predict_probabilities = tprobability[:,1]\n",
    "fpr, tpr, _ = roc_curve(y_testlog, y_predict_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainnn = np.array(y_trainos)\n",
    "y_testnn = np.array(y_testos)\n",
    "X_trainnn = X_trainos\n",
    "X_testnn = X_testos\n",
    "num = [5,10,25,50,75,100,150,200,250]\n",
    "lr = [0.00001,0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "lay = [0,1,2,3,4,5]\n",
    "s = ['sgd','lbfgs','adam']\n",
    "a = [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100]\n",
    "act = ['identity', 'logistic', 'tanh', 'relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for aplha 1e-06: 0.8609019279712722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for aplha 1e-05: 0.8635341491475788\n",
      "training dataset accuracy for aplha 0.0001: 0.8507478297249241\n",
      "training dataset accuracy for aplha 0.001: 0.8340393264302898\n",
      "training dataset accuracy for aplha 0.01: 0.8210699717602761\n",
      "training dataset accuracy for aplha 0.1: 0.8164069309347\n",
      "training dataset accuracy for aplha 1: 0.8072464526025869\n",
      "training dataset accuracy for aplha 10: 0.5\n",
      "training dataset accuracy for aplha 100: 0.5\n"
     ]
    }
   ],
   "source": [
    "#small alpha will be better for a binary classification, alpha is for penalize overfitting\n",
    "for ea in a:\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(200,200,25), random_state=1,alpha=ea)\n",
    "    classifier.fit(X_trainnn, y_trainnn.ravel())\n",
    "    nntrain_result = classifier.predict(X_trainnn)\n",
    "    print ('training dataset accuracy for aplha {}:'.format(ea),accuracy_score(y_trainnn, nntrain_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for solver sgd: 0.7924027902479454\n",
      "training dataset accuracy for solver lbfgs: 0.791946957662822\n",
      "training dataset accuracy for solver adam: 0.8128185648180123\n"
     ]
    }
   ],
   "source": [
    "#Default solver is 'adam', which works pretty well in large dataset, â€˜adamâ€™ refers to a stochastic gradient-based optimizer\n",
    "for esolver in s:\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(100,), random_state=1,solver=esolver,alpha=0.00001)\n",
    "    classifier.fit(X_trainnn, y_trainnn.ravel())\n",
    "    nntrain_result = classifier.predict(X_trainnn)\n",
    "    print ('training dataset accuracy for solver {}:'.format(esolver),accuracy_score(y_trainnn, nntrain_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for activation identity: 0.80417155880931\n",
      "training dataset accuracy for activation logistic: 0.7924027902479454\n",
      "training dataset accuracy for activation tanh: 0.7924442295738656\n",
      "training dataset accuracy for activation relu: 0.8128185648180123\n"
     ]
    }
   ],
   "source": [
    "#test activation\n",
    "for eact in act:\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(100,), random_state=1,activation=eact,alpha=0.00001)\n",
    "    classifier.fit(X_trainnn, y_trainnn.ravel())\n",
    "    nntrain_result = classifier.predict(X_trainnn)\n",
    "    print ('training dataset accuracy for activation {}:'.format(eact),accuracy_score(y_trainnn, nntrain_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for learning rate 1e-05: 0.7927619310725879\n",
      "training dataset accuracy for learning rate 0.001: 0.8128185648180123\n",
      "training dataset accuracy for learning rate 0.01: 0.7926099868775468\n",
      "training dataset accuracy for learning rate 0.1: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.2: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.3: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.4: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.5: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.6: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.7: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.8: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 0.9: 0.7924027902479454\n",
      "training dataset accuracy for learning rate 1: 0.7924027902479454\n"
     ]
    }
   ],
   "source": [
    "for elr in lr:\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(100,), random_state=1,learning_rate_init=elr,alpha=0.00001)\n",
    "    classifier.fit(X_trainnn, y_trainnn.ravel())\n",
    "    nntrain_result = classifier.predict(X_trainnn)\n",
    "    print ('training dataset accuracy for learning rate {}:'.format(elr),accuracy_score(y_trainnn, nntrain_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 5 : 0.8142279398947112\n",
      "testing dataset accuracy for hidden layer sizes 5 : 0.8068129763045018\n",
      "training dataset accuracy for hidden layer sizes 10 : 0.817592302060454\n",
      "testing dataset accuracy for hidden layer sizes 10 : 0.8143089160038313\n",
      "training dataset accuracy for hidden layer sizes 25 : 0.8207823449429976\n",
      "testing dataset accuracy for hidden layer sizes 25 : 0.8213884146087536\n",
      "training dataset accuracy for hidden layer sizes 50 : 0.8219677160687515\n",
      "testing dataset accuracy for hidden layer sizes 50 : 0.8104360138258443\n",
      "training dataset accuracy for hidden layer sizes 75 : 0.8245476414600983\n",
      "testing dataset accuracy for hidden layer sizes 75 : 0.8084787406821305\n",
      "training dataset accuracy for hidden layer sizes 100 : 0.8240944113237806\n",
      "testing dataset accuracy for hidden layer sizes 100 : 0.8110190313580145\n",
      "training dataset accuracy for hidden layer sizes 150 : 0.8222291949935502\n",
      "testing dataset accuracy for hidden layer sizes 150 : 0.8116436929996252\n",
      "training dataset accuracy for hidden layer sizes 200 : 0.826770212320887\n",
      "testing dataset accuracy for hidden layer sizes 200 : 0.8060633823345688\n",
      "training dataset accuracy for hidden layer sizes 250 : 0.8267004846076073\n",
      "testing dataset accuracy for hidden layer sizes 250 : 0.8259692666472328\n"
     ]
    }
   ],
   "source": [
    "for numhiddenunit in num:\n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(numhiddenunit), random_state=1,alpha=0.00001)\n",
    "            classifier.fit(X_trainnn, y_trainnn.ravel())\n",
    "            nntrain_result = classifier.predict(X_trainnn)\n",
    "            nntest_result = classifier.predict(X_testnn)\n",
    "            print ('training dataset accuracy for hidden layer sizes',numhiddenunit,':',accuracy_score(y_trainnn, nntrain_result))\n",
    "            print ('testing dataset accuracy for hidden layer sizes',numhiddenunit,':',accuracy_score(y_testnn, nntest_result))\n",
    "#300 828 822\n",
    "#350 830 822\n",
    "#400 831 816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 5 , 5 : 0.814262803751351\n",
      "testing dataset accuracy for hidden layer sizes 5 , 5 : 0.8024819889226669\n",
      "training dataset accuracy for hidden layer sizes 10 , 5 : 0.8196405536380434\n",
      "testing dataset accuracy for hidden layer sizes 10 , 5 : 0.8069795527422646\n",
      "training dataset accuracy for hidden layer sizes 25 , 5 : 0.8237283408290625\n",
      "testing dataset accuracy for hidden layer sizes 25 , 5 : 0.8187648357139883\n",
      "training dataset accuracy for hidden layer sizes 50 , 5 : 0.8232663947285849\n",
      "testing dataset accuracy for hidden layer sizes 50 , 5 : 0.8102277932786407\n",
      "training dataset accuracy for hidden layer sizes 75 , 5 : 0.8223337865634697\n",
      "testing dataset accuracy for hidden layer sizes 75 , 5 : 0.8038562445342107\n",
      "training dataset accuracy for hidden layer sizes 100 , 5 : 0.8295244569954329\n",
      "testing dataset accuracy for hidden layer sizes 100 , 5 : 0.8141839836755091\n",
      "training dataset accuracy for hidden layer sizes 150 , 5 : 0.8279904473032806\n",
      "testing dataset accuracy for hidden layer sizes 150 , 5 : 0.8107691667013701\n",
      "training dataset accuracy for hidden layer sizes 200 , 5 : 0.834571000244047\n",
      "testing dataset accuracy for hidden layer sizes 200 , 5 : 0.8230125348769417\n",
      "training dataset accuracy for hidden layer sizes 250 , 5 : 0.8338475752187707\n",
      "testing dataset accuracy for hidden layer sizes 250 , 5 : 0.8261358430849957\n",
      "training dataset accuracy for hidden layer sizes 5 , 10 : 0.8137659937942335\n",
      "testing dataset accuracy for hidden layer sizes 5 , 10 : 0.7940282347062008\n",
      "training dataset accuracy for hidden layer sizes 10 , 10 : 0.8188648328278074\n",
      "testing dataset accuracy for hidden layer sizes 10 , 10 : 0.815683171615375\n",
      "training dataset accuracy for hidden layer sizes 25 , 10 : 0.8221071714953108\n",
      "testing dataset accuracy for hidden layer sizes 25 , 10 : 0.8228043143297381\n",
      "training dataset accuracy for hidden layer sizes 50 , 10 : 0.8259509116898511\n",
      "testing dataset accuracy for hidden layer sizes 50 , 10 : 0.8150585099737642\n",
      "training dataset accuracy for hidden layer sizes 75 , 10 : 0.8252884984136946\n",
      "testing dataset accuracy for hidden layer sizes 75 , 10 : 0.8263440636321993\n",
      "training dataset accuracy for hidden layer sizes 100 , 10 : 0.8267789282850468\n",
      "testing dataset accuracy for hidden layer sizes 100 , 10 : 0.8106858784824886\n",
      "training dataset accuracy for hidden layer sizes 150 , 10 : 0.8305093609455078\n",
      "testing dataset accuracy for hidden layer sizes 150 , 10 : 0.8373381085245492\n",
      "training dataset accuracy for hidden layer sizes 200 , 10 : 0.8328713872328557\n",
      "testing dataset accuracy for hidden layer sizes 200 , 10 : 0.8234706200807895\n",
      "training dataset accuracy for hidden layer sizes 250 , 10 : 0.8318516194261409\n",
      "testing dataset accuracy for hidden layer sizes 250 , 10 : 0.8305501186857119\n",
      "training dataset accuracy for hidden layer sizes 5 , 25 : 0.817958372555172\n",
      "testing dataset accuracy for hidden layer sizes 5 , 25 : 0.8141006954566277\n",
      "training dataset accuracy for hidden layer sizes 10 , 25 : 0.8207997768713176\n",
      "testing dataset accuracy for hidden layer sizes 10 , 25 : 0.8240536376129597\n",
      "training dataset accuracy for hidden layer sizes 25 , 25 : 0.8243384583202594\n",
      "testing dataset accuracy for hidden layer sizes 25 , 25 : 0.8201807354349727\n",
      "training dataset accuracy for hidden layer sizes 50 , 25 : 0.8269183837116062\n",
      "testing dataset accuracy for hidden layer sizes 50 , 25 : 0.8172240036646816\n",
      "training dataset accuracy for hidden layer sizes 75 , 25 : 0.8277812641634418\n",
      "testing dataset accuracy for hidden layer sizes 75 , 25 : 0.8194727855744804\n",
      "training dataset accuracy for hidden layer sizes 100 , 25 : 0.8249398598472963\n",
      "testing dataset accuracy for hidden layer sizes 100 , 25 : 0.8173905801024445\n",
      "training dataset accuracy for hidden layer sizes 150 , 25 : 0.8356169159432416\n",
      "testing dataset accuracy for hidden layer sizes 150 , 25 : 0.8255944696622662\n",
      "training dataset accuracy for hidden layer sizes 200 , 25 : 0.8372903810619531\n",
      "testing dataset accuracy for hidden layer sizes 200 , 25 : 0.8243867904884854\n",
      "training dataset accuracy for hidden layer sizes 250 , 25 : 0.8355210403374821\n",
      "testing dataset accuracy for hidden layer sizes 250 , 25 : 0.8273851663682172\n",
      "training dataset accuracy for hidden layer sizes 5 , 50 : 0.8173395390998153\n",
      "testing dataset accuracy for hidden layer sizes 5 , 50 : 0.8115604047807438\n",
      "training dataset accuracy for hidden layer sizes 10 , 50 : 0.8177404734511732\n",
      "testing dataset accuracy for hidden layer sizes 10 , 50 : 0.814933577645442\n",
      "training dataset accuracy for hidden layer sizes 25 , 50 : 0.8243907541052191\n",
      "testing dataset accuracy for hidden layer sizes 25 , 50 : 0.8126847957356432\n",
      "training dataset accuracy for hidden layer sizes 50 , 50 : 0.8219590001045916\n",
      "testing dataset accuracy for hidden layer sizes 50 , 50 : 0.8132261691583725\n",
      "training dataset accuracy for hidden layer sizes 75 , 50 : 0.8326360562005369\n",
      "testing dataset accuracy for hidden layer sizes 75 , 50 : 0.8236371965185525\n",
      "training dataset accuracy for hidden layer sizes 100 , 50 : 0.8317121639995816\n",
      "testing dataset accuracy for hidden layer sizes 100 , 50 : 0.8182234622912589\n",
      "training dataset accuracy for hidden layer sizes 150 , 50 : 0.836532092180037\n",
      "testing dataset accuracy for hidden layer sizes 150 , 50 : 0.820722108857702\n",
      "training dataset accuracy for hidden layer sizes 200 , 50 : 0.8390684377505839\n",
      "testing dataset accuracy for hidden layer sizes 200 , 50 : 0.826219131303877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 50 : 0.8443328801031971\n",
      "testing dataset accuracy for hidden layer sizes 250 , 50 : 0.817473868321326\n",
      "training dataset accuracy for hidden layer sizes 5 , 75 : 0.8217585329289125\n",
      "testing dataset accuracy for hidden layer sizes 5 , 75 : 0.8063548911006538\n",
      "training dataset accuracy for hidden layer sizes 10 , 75 : 0.8226998570581878\n",
      "testing dataset accuracy for hidden layer sizes 10 , 75 : 0.8100612168408778\n",
      "training dataset accuracy for hidden layer sizes 25 , 75 : 0.8221333193877907\n",
      "testing dataset accuracy for hidden layer sizes 25 , 75 : 0.8269270811643693\n",
      "training dataset accuracy for hidden layer sizes 50 , 75 : 0.8238765122197818\n",
      "testing dataset accuracy for hidden layer sizes 50 , 75 : 0.8160163244909008\n",
      "training dataset accuracy for hidden layer sizes 75 , 75 : 0.8227260049506676\n",
      "testing dataset accuracy for hidden layer sizes 75 , 75 : 0.8191396326989547\n",
      "training dataset accuracy for hidden layer sizes 100 , 75 : 0.8299515392392707\n",
      "testing dataset accuracy for hidden layer sizes 100 , 75 : 0.828301336775913\n",
      "training dataset accuracy for hidden layer sizes 150 , 75 : 0.8325750444514172\n",
      "testing dataset accuracy for hidden layer sizes 150 , 75 : 0.8170574272269188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 75 : 0.8467994979604644\n",
      "testing dataset accuracy for hidden layer sizes 200 , 75 : 0.8227210261108566\n",
      "training dataset accuracy for hidden layer sizes 250 , 75 : 0.8466949063905449\n",
      "testing dataset accuracy for hidden layer sizes 250 , 75 : 0.8215549910465165\n",
      "training dataset accuracy for hidden layer sizes 5 , 100 : 0.8169996164975769\n",
      "testing dataset accuracy for hidden layer sizes 5 , 100 : 0.803356515220922\n",
      "training dataset accuracy for hidden layer sizes 10 , 100 : 0.8217411010005927\n",
      "testing dataset accuracy for hidden layer sizes 10 , 100 : 0.8064381793195352\n",
      "training dataset accuracy for hidden layer sizes 25 , 100 : 0.8280863229090402\n",
      "testing dataset accuracy for hidden layer sizes 25 , 100 : 0.8243035022696039\n",
      "training dataset accuracy for hidden layer sizes 50 , 100 : 0.8288097479343165\n",
      "testing dataset accuracy for hidden layer sizes 50 , 100 : 0.8233456877524674\n",
      "training dataset accuracy for hidden layer sizes 75 , 100 : 0.8357912352264407\n",
      "testing dataset accuracy for hidden layer sizes 75 , 100 : 0.8245117228168075\n",
      "training dataset accuracy for hidden layer sizes 100 , 100 : 0.8338737231112505\n",
      "testing dataset accuracy for hidden layer sizes 100 , 100 : 0.814933577645442\n",
      "training dataset accuracy for hidden layer sizes 150 , 100 : 0.8354861764808423\n",
      "testing dataset accuracy for hidden layer sizes 150 , 100 : 0.8243035022696039\n",
      "training dataset accuracy for hidden layer sizes 200 , 100 : 0.8420841613499286\n",
      "testing dataset accuracy for hidden layer sizes 200 , 100 : 0.8242618581601633\n",
      "training dataset accuracy for hidden layer sizes 250 , 100 : 0.8424066520238469\n",
      "testing dataset accuracy for hidden layer sizes 250 , 100 : 0.826677216507725\n",
      "training dataset accuracy for hidden layer sizes 5 , 150 : 0.8194923822473242\n",
      "testing dataset accuracy for hidden layer sizes 5 , 150 : 0.805147211926873\n",
      "training dataset accuracy for hidden layer sizes 10 , 150 : 0.8199630443119618\n",
      "testing dataset accuracy for hidden layer sizes 10 , 150 : 0.8230958230958231\n",
      "training dataset accuracy for hidden layer sizes 25 , 150 : 0.8228131645922672\n",
      "testing dataset accuracy for hidden layer sizes 25 , 150 : 0.7981093574313913\n",
      "training dataset accuracy for hidden layer sizes 50 , 150 : 0.8365582400725168\n",
      "testing dataset accuracy for hidden layer sizes 50 , 150 : 0.8249281639112147\n",
      "training dataset accuracy for hidden layer sizes 75 , 150 : 0.8290014991458355\n",
      "testing dataset accuracy for hidden layer sizes 75 , 150 : 0.8088118935576563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 100 , 150 : 0.8454921033364711\n",
      "testing dataset accuracy for hidden layer sizes 100 , 150 : 0.8197642943405655\n",
      "training dataset accuracy for hidden layer sizes 150 , 150 : 0.8383014329045079\n",
      "testing dataset accuracy for hidden layer sizes 150 , 150 : 0.8188897680423104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 150 : 0.8507303977966043\n",
      "testing dataset accuracy for hidden layer sizes 200 , 150 : 0.826677216507725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 150 : 0.853711257539309\n",
      "testing dataset accuracy for hidden layer sizes 250 , 150 : 0.8161829009286636\n",
      "training dataset accuracy for hidden layer sizes 5 , 200 : 0.8147247498518286\n",
      "testing dataset accuracy for hidden layer sizes 5 , 200 : 0.817015783117478\n",
      "training dataset accuracy for hidden layer sizes 10 , 200 : 0.824007251682181\n",
      "testing dataset accuracy for hidden layer sizes 10 , 200 : 0.8139757631283056\n",
      "training dataset accuracy for hidden layer sizes 25 , 200 : 0.8251577589512952\n",
      "testing dataset accuracy for hidden layer sizes 25 , 200 : 0.809436555199267\n",
      "training dataset accuracy for hidden layer sizes 50 , 200 : 0.8310933305442249\n",
      "testing dataset accuracy for hidden layer sizes 50 , 200 : 0.8228459584391787\n",
      "training dataset accuracy for hidden layer sizes 75 , 200 : 0.839652407349301\n",
      "testing dataset accuracy for hidden layer sizes 75 , 200 : 0.8268854370549286\n",
      "training dataset accuracy for hidden layer sizes 100 , 200 : 0.8383798765819475\n",
      "testing dataset accuracy for hidden layer sizes 100 , 200 : 0.8268437929454878\n",
      "training dataset accuracy for hidden layer sizes 150 , 200 : 0.8470435449569431\n",
      "testing dataset accuracy for hidden layer sizes 150 , 200 : 0.8220130762503643\n",
      "training dataset accuracy for hidden layer sizes 200 , 200 : 0.8482899278318168\n",
      "testing dataset accuracy for hidden layer sizes 200 , 200 : 0.8286344896514388\n",
      "training dataset accuracy for hidden layer sizes 250 , 200 : 0.8418836941742496\n",
      "testing dataset accuracy for hidden layer sizes 250 , 200 : 0.8218464998126015\n",
      "training dataset accuracy for hidden layer sizes 5 , 250 : 0.8162936234006206\n",
      "testing dataset accuracy for hidden layer sizes 5 , 250 : 0.8100612168408778\n",
      "training dataset accuracy for hidden layer sizes 10 , 250 : 0.8215406338249137\n",
      "testing dataset accuracy for hidden layer sizes 10 , 250 : 0.8114354724524216\n",
      "training dataset accuracy for hidden layer sizes 25 , 250 : 0.8229787679113063\n",
      "testing dataset accuracy for hidden layer sizes 25 , 250 : 0.8253029608961813\n",
      "training dataset accuracy for hidden layer sizes 50 , 250 : 0.8338214273262908\n",
      "testing dataset accuracy for hidden layer sizes 50 , 250 : 0.8171407154458001\n",
      "training dataset accuracy for hidden layer sizes 75 , 250 : 0.8401492173064184\n",
      "testing dataset accuracy for hidden layer sizes 75 , 250 : 0.8162245450381044\n",
      "training dataset accuracy for hidden layer sizes 100 , 250 : 0.8440016734651187\n",
      "testing dataset accuracy for hidden layer sizes 100 , 250 : 0.8181818181818182\n",
      "training dataset accuracy for hidden layer sizes 150 , 250 : 0.8407941986542551\n",
      "testing dataset accuracy for hidden layer sizes 150 , 250 : 0.822221296797568\n",
      "training dataset accuracy for hidden layer sizes 200 , 250 : 0.846102220827668\n",
      "testing dataset accuracy for hidden layer sizes 200 , 250 : 0.8176820888685296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 250 : 0.8548791967367431\n",
      "testing dataset accuracy for hidden layer sizes 250 , 250 : 0.8175988006496481\n"
     ]
    }
   ],
   "source": [
    "for eachn in num:\n",
    "        for numhiddenunit in num:\n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(numhiddenunit,eachn), random_state=1,alpha=0.00001)\n",
    "            classifier.fit(X_trainnn, y_trainnn.ravel())\n",
    "            nntrain_result = classifier.predict(X_trainnn)\n",
    "            nntest_result = classifier.predict(X_testnn)\n",
    "            print ('training dataset accuracy for hidden layer sizes',numhiddenunit,',',eachn,':',accuracy_score(y_trainnn, nntrain_result))\n",
    "            print ('testing dataset accuracy for hidden layer sizes',numhiddenunit,',',eachn,':',accuracy_score(y_testnn, nntest_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 5 , 5 , 5 : 0.8157968134435031\n",
      "testing dataset accuracy for hidden layer sizes 5 , 5 , 5 : 0.809436555199267\n",
      "training dataset accuracy for hidden layer sizes 10 , 5 , 5 : 0.8215232018965938\n",
      "testing dataset accuracy for hidden layer sizes 10 , 5 , 5 : 0.8179735976346146\n",
      "training dataset accuracy for hidden layer sizes 25 , 5 , 5 : 0.8177404734511732\n",
      "testing dataset accuracy for hidden layer sizes 25 , 5 , 5 : 0.8038562445342107\n",
      "training dataset accuracy for hidden layer sizes 50 , 5 , 5 : 0.8241031272879406\n",
      "testing dataset accuracy for hidden layer sizes 50 , 5 , 5 : 0.8261774871944364\n",
      "training dataset accuracy for hidden layer sizes 75 , 5 , 5 : 0.8233971341909843\n",
      "testing dataset accuracy for hidden layer sizes 75 , 5 , 5 : 0.8080622995877234\n",
      "training dataset accuracy for hidden layer sizes 100 , 5 , 5 : 0.5\n",
      "testing dataset accuracy for hidden layer sizes 100 , 5 , 5 : 0.20413942447840752\n",
      "training dataset accuracy for hidden layer sizes 150 , 5 , 5 : 0.8332723215842136\n",
      "testing dataset accuracy for hidden layer sizes 150 , 5 , 5 : 0.8290925748552868\n",
      "training dataset accuracy for hidden layer sizes 200 , 5 , 5 : 0.8268835198549663\n",
      "testing dataset accuracy for hidden layer sizes 200 , 5 , 5 : 0.8060633823345688\n",
      "training dataset accuracy for hidden layer sizes 250 , 5 , 5 : 0.829480877174633\n",
      "testing dataset accuracy for hidden layer sizes 250 , 5 , 5 : 0.8352142589430726\n",
      "training dataset accuracy for hidden layer sizes 5 , 10 , 5 : 0.815047240525747\n",
      "testing dataset accuracy for hidden layer sizes 5 , 10 , 5 : 0.7996085453712573\n",
      "training dataset accuracy for hidden layer sizes 10 , 10 , 5 : 0.8191611756092458\n",
      "testing dataset accuracy for hidden layer sizes 10 , 10 , 5 : 0.8200558031066506\n",
      "training dataset accuracy for hidden layer sizes 25 , 10 , 5 : 0.8241118432521005\n",
      "testing dataset accuracy for hidden layer sizes 25 , 10 , 5 : 0.8205555324199392\n",
      "training dataset accuracy for hidden layer sizes 50 , 10 , 5 : 0.8268399400341666\n",
      "testing dataset accuracy for hidden layer sizes 50 , 10 , 5 : 0.8257194019905885\n",
      "training dataset accuracy for hidden layer sizes 75 , 10 , 5 : 0.8303699055189485\n",
      "testing dataset accuracy for hidden layer sizes 75 , 10 , 5 : 0.8242202140507225\n",
      "training dataset accuracy for hidden layer sizes 100 , 10 , 5 : 0.8313373775407036\n",
      "testing dataset accuracy for hidden layer sizes 100 , 10 , 5 : 0.829509015949694\n",
      "training dataset accuracy for hidden layer sizes 150 , 10 , 5 : 0.829881811525991\n",
      "testing dataset accuracy for hidden layer sizes 150 , 10 , 5 : 0.8215966351559572\n",
      "training dataset accuracy for hidden layer sizes 200 , 10 , 5 : 0.8390248579297842\n",
      "testing dataset accuracy for hidden layer sizes 200 , 10 , 5 : 0.8248448756923333\n",
      "training dataset accuracy for hidden layer sizes 250 , 10 , 5 : 0.8280165951957605\n",
      "testing dataset accuracy for hidden layer sizes 250 , 10 , 5 : 0.8153500187398492\n",
      "training dataset accuracy for hidden layer sizes 5 , 25 , 5 : 0.8157358016943834\n",
      "testing dataset accuracy for hidden layer sizes 5 , 25 , 5 : 0.8084787406821305\n",
      "training dataset accuracy for hidden layer sizes 10 , 25 , 5 : 0.8227172889865076\n",
      "testing dataset accuracy for hidden layer sizes 10 , 25 , 5 : 0.810477657935285\n",
      "training dataset accuracy for hidden layer sizes 25 , 25 , 5 : 0.8236586131157829\n",
      "testing dataset accuracy for hidden layer sizes 25 , 25 , 5 : 0.8187648357139883\n",
      "training dataset accuracy for hidden layer sizes 50 , 25 , 5 : 0.8260467872956107\n",
      "testing dataset accuracy for hidden layer sizes 50 , 25 , 5 : 0.8116020488901845\n",
      "training dataset accuracy for hidden layer sizes 75 , 25 , 5 : 0.831842903461981\n",
      "testing dataset accuracy for hidden layer sizes 75 , 25 , 5 : 0.8223462291258902\n",
      "training dataset accuracy for hidden layer sizes 100 , 25 , 5 : 0.5\n",
      "testing dataset accuracy for hidden layer sizes 100 , 25 , 5 : 0.20413942447840752\n",
      "training dataset accuracy for hidden layer sizes 150 , 25 , 5 : 0.8352508454485236\n",
      "testing dataset accuracy for hidden layer sizes 150 , 25 , 5 : 0.8303002540290676\n",
      "training dataset accuracy for hidden layer sizes 200 , 25 , 5 : 0.8407680507617753\n",
      "testing dataset accuracy for hidden layer sizes 200 , 25 , 5 : 0.8285095573231166\n",
      "training dataset accuracy for hidden layer sizes 250 , 25 , 5 : 0.8411689851131332\n",
      "testing dataset accuracy for hidden layer sizes 250 , 25 , 5 : 0.8144754924415941\n",
      "training dataset accuracy for hidden layer sizes 5 , 50 , 5 : 0.8140013248265523\n",
      "testing dataset accuracy for hidden layer sizes 5 , 50 , 5 : 0.808103943697164\n",
      "training dataset accuracy for hidden layer sizes 10 , 50 , 5 : 0.8214796220757941\n",
      "testing dataset accuracy for hidden layer sizes 10 , 50 , 5 : 0.8186399033856661\n",
      "training dataset accuracy for hidden layer sizes 25 , 50 , 5 : 0.8328975351253356\n",
      "testing dataset accuracy for hidden layer sizes 25 , 50 , 5 : 0.818806479823429\n",
      "training dataset accuracy for hidden layer sizes 50 , 50 , 5 : 0.827920719590001\n",
      "testing dataset accuracy for hidden layer sizes 50 , 50 , 5 : 0.8157664598342564\n",
      "training dataset accuracy for hidden layer sizes 75 , 50 , 5 : 0.8345361363874072\n",
      "testing dataset accuracy for hidden layer sizes 75 , 50 , 5 : 0.8217215674842794\n",
      "training dataset accuracy for hidden layer sizes 100 , 50 , 5 : 0.833002126695255\n",
      "testing dataset accuracy for hidden layer sizes 100 , 50 , 5 : 0.8180152417440553\n",
      "training dataset accuracy for hidden layer sizes 150 , 50 , 5 : 0.8359742704737998\n",
      "testing dataset accuracy for hidden layer sizes 150 , 50 , 5 : 0.814642068879357\n",
      "training dataset accuracy for hidden layer sizes 200 , 50 , 5 : 0.8379353624097897\n",
      "testing dataset accuracy for hidden layer sizes 200 , 50 , 5 : 0.8093532669803857\n",
      "training dataset accuracy for hidden layer sizes 250 , 50 , 5 : 0.844141128891678\n",
      "testing dataset accuracy for hidden layer sizes 250 , 50 , 5 : 0.8266355723982842\n",
      "training dataset accuracy for hidden layer sizes 5 , 75 , 5 : 0.8174702785622145\n",
      "testing dataset accuracy for hidden layer sizes 5 , 75 , 5 : 0.809436555199267\n",
      "training dataset accuracy for hidden layer sizes 10 , 75 , 5 : 0.8196667015305233\n",
      "testing dataset accuracy for hidden layer sizes 10 , 75 , 5 : 0.8114354724524216\n",
      "training dataset accuracy for hidden layer sizes 25 , 75 , 5 : 0.8262908342920894\n",
      "testing dataset accuracy for hidden layer sizes 25 , 75 , 5 : 0.8164744096947487\n",
      "training dataset accuracy for hidden layer sizes 50 , 75 , 5 : 0.823449429975944\n",
      "testing dataset accuracy for hidden layer sizes 50 , 75 , 5 : 0.8305917627951527\n",
      "training dataset accuracy for hidden layer sizes 75 , 75 , 5 : 0.8311369103650246\n",
      "testing dataset accuracy for hidden layer sizes 75 , 75 , 5 : 0.8232623995335859\n",
      "training dataset accuracy for hidden layer sizes 100 , 75 , 5 : 0.8362793292193983\n",
      "testing dataset accuracy for hidden layer sizes 100 , 75 , 5 : 0.833215341689918\n",
      "training dataset accuracy for hidden layer sizes 150 , 75 , 5 : 0.841395600181292\n",
      "testing dataset accuracy for hidden layer sizes 150 , 75 , 5 : 0.8216799233748386\n",
      "training dataset accuracy for hidden layer sizes 200 , 75 , 5 : 0.8421626050273682\n",
      "testing dataset accuracy for hidden layer sizes 200 , 75 , 5 : 0.8172656477741224\n",
      "training dataset accuracy for hidden layer sizes 250 , 75 , 5 : 0.8381096816929888\n",
      "testing dataset accuracy for hidden layer sizes 250 , 75 , 5 : 0.802315412484904\n",
      "training dataset accuracy for hidden layer sizes 5 , 100 , 5 : 0.8216626573231531\n",
      "testing dataset accuracy for hidden layer sizes 5 , 100 , 5 : 0.8018989713904968\n",
      "training dataset accuracy for hidden layer sizes 10 , 100 , 5 : 0.8235365896175435\n",
      "testing dataset accuracy for hidden layer sizes 10 , 100 , 5 : 0.8152250864115271\n",
      "training dataset accuracy for hidden layer sizes 25 , 100 , 5 : 0.8246696649583377\n",
      "testing dataset accuracy for hidden layer sizes 25 , 100 , 5 : 0.8332569857993587\n",
      "training dataset accuracy for hidden layer sizes 50 , 100 , 5 : 0.8364972283233971\n",
      "testing dataset accuracy for hidden layer sizes 50 , 100 , 5 : 0.8175988006496481\n",
      "training dataset accuracy for hidden layer sizes 75 , 100 , 5 : 0.8247568245999373\n",
      "testing dataset accuracy for hidden layer sizes 75 , 100 , 5 : 0.81539166284929\n",
      "training dataset accuracy for hidden layer sizes 100 , 100 , 5 : 0.8282606421922393\n",
      "testing dataset accuracy for hidden layer sizes 100 , 100 , 5 : 0.8204722442010578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 150 , 100 , 5 : 0.8333159014050134\n",
      "testing dataset accuracy for hidden layer sizes 150 , 100 , 5 : 0.8104360138258443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 100 , 5 : 0.8529529686573929\n",
      "testing dataset accuracy for hidden layer sizes 200 , 100 , 5 : 0.8228876025486195\n",
      "training dataset accuracy for hidden layer sizes 250 , 100 , 5 : 0.8488477495380539\n",
      "testing dataset accuracy for hidden layer sizes 250 , 100 , 5 : 0.8239703493940782\n",
      "training dataset accuracy for hidden layer sizes 5 , 150 , 5 : 0.8159885646550221\n",
      "testing dataset accuracy for hidden layer sizes 5 , 150 , 5 : 0.8022321242660225\n",
      "training dataset accuracy for hidden layer sizes 10 , 150 , 5 : 0.8205993096956385\n",
      "testing dataset accuracy for hidden layer sizes 10 , 150 , 5 : 0.813767542581102\n",
      "training dataset accuracy for hidden layer sizes 25 , 150 , 5 : 0.8295157410312729\n",
      "testing dataset accuracy for hidden layer sizes 25 , 150 , 5 : 0.8063132469912131\n",
      "training dataset accuracy for hidden layer sizes 50 , 150 , 5 : 0.8317644597845414\n",
      "testing dataset accuracy for hidden layer sizes 50 , 150 , 5 : 0.8259276225377921\n",
      "training dataset accuracy for hidden layer sizes 75 , 150 , 5 : 0.8357825192622808\n",
      "testing dataset accuracy for hidden layer sizes 75 , 150 , 5 : 0.8166409861325116\n",
      "training dataset accuracy for hidden layer sizes 100 , 150 , 5 : 0.8463549837883066\n",
      "testing dataset accuracy for hidden layer sizes 100 , 150 , 5 : 0.8283429808853537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 150 , 150 , 5 : 0.8496844820974097\n",
      "testing dataset accuracy for hidden layer sizes 150 , 150 , 5 : 0.7985257985257985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 150 , 5 : 0.85548059826378\n",
      "testing dataset accuracy for hidden layer sizes 200 , 150 , 5 : 0.8211385499521092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 150 , 5 : 0.85742425827145\n",
      "testing dataset accuracy for hidden layer sizes 250 , 150 , 5 : 0.825178028567859\n",
      "training dataset accuracy for hidden layer sizes 5 , 200 , 5 : 0.8136091064393544\n",
      "testing dataset accuracy for hidden layer sizes 5 , 200 , 5 : 0.7916961645775205\n",
      "training dataset accuracy for hidden layer sizes 10 , 200 , 5 : 0.825985775546491\n",
      "testing dataset accuracy for hidden layer sizes 10 , 200 , 5 : 0.8131845250489318\n",
      "training dataset accuracy for hidden layer sizes 25 , 200 , 5 : 0.8247306767074574\n",
      "testing dataset accuracy for hidden layer sizes 25 , 200 , 5 : 0.8188897680423104\n",
      "training dataset accuracy for hidden layer sizes 50 , 200 , 5 : 0.8281909144789596\n",
      "testing dataset accuracy for hidden layer sizes 50 , 200 , 5 : 0.8313413567650856\n",
      "training dataset accuracy for hidden layer sizes 75 , 200 , 5 : 0.842598403235366\n",
      "testing dataset accuracy for hidden layer sizes 75 , 200 , 5 : 0.8141839836755091\n",
      "training dataset accuracy for hidden layer sizes 100 , 200 , 5 : 0.839451940173622\n",
      "testing dataset accuracy for hidden layer sizes 100 , 200 , 5 : 0.8218881439220422\n",
      "training dataset accuracy for hidden layer sizes 150 , 200 , 5 : 0.8504514869434857\n",
      "testing dataset accuracy for hidden layer sizes 150 , 200 , 5 : 0.8158497480531379\n",
      "training dataset accuracy for hidden layer sizes 200 , 200 , 5 : 0.8403322525537775\n",
      "testing dataset accuracy for hidden layer sizes 200 , 200 , 5 : 0.8185982592762254\n",
      "training dataset accuracy for hidden layer sizes 250 , 200 , 5 : 0.8533451870445908\n",
      "testing dataset accuracy for hidden layer sizes 250 , 200 , 5 : 0.8162245450381044\n",
      "training dataset accuracy for hidden layer sizes 5 , 250 , 5 : 0.8150821043823868\n",
      "testing dataset accuracy for hidden layer sizes 5 , 250 , 5 : 0.7981926456502728\n",
      "training dataset accuracy for hidden layer sizes 10 , 250 , 5 : 0.823641181187463\n",
      "testing dataset accuracy for hidden layer sizes 10 , 250 , 5 : 0.8052721442551951\n",
      "training dataset accuracy for hidden layer sizes 25 , 250 , 5 : 0.823649897151623\n",
      "testing dataset accuracy for hidden layer sizes 25 , 250 , 5 : 0.814933577645442\n",
      "training dataset accuracy for hidden layer sizes 50 , 250 , 5 : 0.8319649269602203\n",
      "testing dataset accuracy for hidden layer sizes 50 , 250 , 5 : 0.8037729563153292\n",
      "training dataset accuracy for hidden layer sizes 75 , 250 , 5 : 0.829698776278632\n",
      "testing dataset accuracy for hidden layer sizes 75 , 250 , 5 : 0.8129346603922875\n",
      "training dataset accuracy for hidden layer sizes 100 , 250 , 5 : 0.8446902346337551\n",
      "testing dataset accuracy for hidden layer sizes 100 , 250 , 5 : 0.8238454170657561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 150 , 250 , 5 : 0.8575811456263291\n",
      "testing dataset accuracy for hidden layer sizes 150 , 250 , 5 : 0.8260941989755549\n",
      "training dataset accuracy for hidden layer sizes 200 , 250 , 5 : 0.8533626189729108\n",
      "testing dataset accuracy for hidden layer sizes 200 , 250 , 5 : 0.8165576979136301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 250 , 5 : 0.8613115782867901\n",
      "testing dataset accuracy for hidden layer sizes 250 , 250 , 5 : 0.8240536376129597\n",
      "training dataset accuracy for hidden layer sizes 5 , 5 , 10 : 0.8146201582819091\n",
      "testing dataset accuracy for hidden layer sizes 5 , 5 , 10 : 0.8040644650814143\n",
      "training dataset accuracy for hidden layer sizes 10 , 5 , 10 : 0.8200240560610815\n",
      "testing dataset accuracy for hidden layer sizes 10 , 5 , 10 : 0.8102694373880814\n",
      "training dataset accuracy for hidden layer sizes 25 , 5 , 10 : 0.8241641390370603\n",
      "testing dataset accuracy for hidden layer sizes 25 , 5 , 10 : 0.8212218381709907\n",
      "training dataset accuracy for hidden layer sizes 50 , 5 , 10 : 0.8265871770735279\n",
      "testing dataset accuracy for hidden layer sizes 50 , 5 , 10 : 0.8092699787615042\n",
      "training dataset accuracy for hidden layer sizes 75 , 5 , 10 : 0.8262995502562493\n",
      "testing dataset accuracy for hidden layer sizes 75 , 5 , 10 : 0.8138508307999833\n",
      "training dataset accuracy for hidden layer sizes 100 , 5 , 10 : 0.8268835198549663\n",
      "testing dataset accuracy for hidden layer sizes 100 , 5 , 10 : 0.8193478532461583\n",
      "training dataset accuracy for hidden layer sizes 150 , 5 , 10 : 0.8282955060488791\n",
      "testing dataset accuracy for hidden layer sizes 150 , 5 , 10 : 0.8121017782034731\n",
      "training dataset accuracy for hidden layer sizes 200 , 5 , 10 : 0.8319300631035805\n",
      "testing dataset accuracy for hidden layer sizes 200 , 5 , 10 : 0.8206804647482614\n",
      "training dataset accuracy for hidden layer sizes 250 , 5 , 10 : 0.8332810375483736\n",
      "testing dataset accuracy for hidden layer sizes 250 , 5 , 10 : 0.8192229209178362\n",
      "training dataset accuracy for hidden layer sizes 5 , 10 , 10 : 0.8161890318307011\n",
      "testing dataset accuracy for hidden layer sizes 5 , 10 , 10 : 0.8062716028817724\n",
      "training dataset accuracy for hidden layer sizes 10 , 10 , 10 : 0.8196841334588432\n",
      "testing dataset accuracy for hidden layer sizes 10 , 10 , 10 : 0.8063548911006538\n",
      "training dataset accuracy for hidden layer sizes 25 , 10 , 10 : 0.8259160478332113\n",
      "testing dataset accuracy for hidden layer sizes 25 , 10 , 10 : 0.8235955524091118\n",
      "training dataset accuracy for hidden layer sizes 50 , 10 , 10 : 0.8253233622703343\n",
      "testing dataset accuracy for hidden layer sizes 50 , 10 , 10 : 0.8069379086328239\n",
      "training dataset accuracy for hidden layer sizes 75 , 10 , 10 : 0.8235191576892236\n",
      "testing dataset accuracy for hidden layer sizes 75 , 10 , 10 : 0.8283846249947945\n",
      "training dataset accuracy for hidden layer sizes 100 , 10 , 10 : 0.8292019663215144\n",
      "testing dataset accuracy for hidden layer sizes 100 , 10 , 10 : 0.813600966143339\n",
      "training dataset accuracy for hidden layer sizes 150 , 10 , 10 : 0.8299515392392707\n",
      "testing dataset accuracy for hidden layer sizes 150 , 10 , 10 : 0.828301336775913\n",
      "training dataset accuracy for hidden layer sizes 200 , 10 , 10 : 0.8301432904507897\n",
      "testing dataset accuracy for hidden layer sizes 200 , 10 , 10 : 0.8046474826135843\n",
      "training dataset accuracy for hidden layer sizes 250 , 10 , 10 : 0.8409946658299341\n",
      "testing dataset accuracy for hidden layer sizes 250 , 10 , 10 : 0.8303418981385083\n",
      "training dataset accuracy for hidden layer sizes 5 , 25 , 10 : 0.8177840532719729\n",
      "testing dataset accuracy for hidden layer sizes 5 , 25 , 10 : 0.7979844251030692\n",
      "training dataset accuracy for hidden layer sizes 10 , 25 , 10 : 0.826578461109368\n",
      "testing dataset accuracy for hidden layer sizes 10 , 25 , 10 : 0.8192645650272768\n",
      "training dataset accuracy for hidden layer sizes 25 , 25 , 10 : 0.8251403270229752\n",
      "testing dataset accuracy for hidden layer sizes 25 , 25 , 10 : 0.8148086453171199\n",
      "training dataset accuracy for hidden layer sizes 50 , 25 , 10 : 0.8248439842415368\n",
      "testing dataset accuracy for hidden layer sizes 50 , 25 , 10 : 0.807812434931079\n",
      "training dataset accuracy for hidden layer sizes 75 , 25 , 10 : 0.8301868702715894\n",
      "testing dataset accuracy for hidden layer sizes 75 , 25 , 10 : 0.8238037729563153\n",
      "training dataset accuracy for hidden layer sizes 100 , 25 , 10 : 0.8323309974549384\n",
      "testing dataset accuracy for hidden layer sizes 100 , 25 , 10 : 0.8278848956815058\n",
      "training dataset accuracy for hidden layer sizes 150 , 25 , 10 : 0.8310323187951051\n",
      "testing dataset accuracy for hidden layer sizes 150 , 25 , 10 : 0.8014408861866489\n",
      "training dataset accuracy for hidden layer sizes 200 , 25 , 10 : 0.8367064114632361\n",
      "testing dataset accuracy for hidden layer sizes 200 , 25 , 10 : 0.8227626702202974\n",
      "training dataset accuracy for hidden layer sizes 250 , 25 , 10 : 0.833202593870934\n",
      "testing dataset accuracy for hidden layer sizes 250 , 25 , 10 : 0.8260525548661142\n",
      "training dataset accuracy for hidden layer sizes 5 , 50 , 10 : 0.8201809434159607\n",
      "testing dataset accuracy for hidden layer sizes 5 , 50 , 10 : 0.8169324948985965\n",
      "training dataset accuracy for hidden layer sizes 10 , 50 , 10 : 0.8227521528431475\n",
      "testing dataset accuracy for hidden layer sizes 10 , 50 , 10 : 0.8129763045017282\n",
      "training dataset accuracy for hidden layer sizes 25 , 50 , 10 : 0.8230310636962661\n",
      "testing dataset accuracy for hidden layer sizes 25 , 50 , 10 : 0.8131845250489318\n",
      "training dataset accuracy for hidden layer sizes 50 , 50 , 10 : 0.8282867900847192\n",
      "testing dataset accuracy for hidden layer sizes 50 , 50 , 10 : 0.8217632115937201\n",
      "training dataset accuracy for hidden layer sizes 75 , 50 , 10 : 0.8260119234389708\n",
      "testing dataset accuracy for hidden layer sizes 75 , 50 , 10 : 0.8185149710573439\n",
      "training dataset accuracy for hidden layer sizes 100 , 50 , 10 : 0.8332636056200536\n",
      "testing dataset accuracy for hidden layer sizes 100 , 50 , 10 : 0.8139757631283056\n",
      "training dataset accuracy for hidden layer sizes 150 , 50 , 10 : 0.8326709200571767\n",
      "testing dataset accuracy for hidden layer sizes 150 , 50 , 10 : 0.8248865198017741\n",
      "training dataset accuracy for hidden layer sizes 200 , 50 , 10 : 0.8445594951713559\n",
      "testing dataset accuracy for hidden layer sizes 200 , 50 , 10 : 0.8154749510681714\n",
      "training dataset accuracy for hidden layer sizes 250 , 50 , 10 : 0.8336906878638916\n",
      "testing dataset accuracy for hidden layer sizes 250 , 50 , 10 : 0.8163911214758672\n",
      "training dataset accuracy for hidden layer sizes 5 , 75 , 10 : 0.8211484154377158\n",
      "testing dataset accuracy for hidden layer sizes 5 , 75 , 10 : 0.8191396326989547\n",
      "training dataset accuracy for hidden layer sizes 10 , 75 , 10 : 0.82227277481435\n",
      "testing dataset accuracy for hidden layer sizes 10 , 75 , 10 : 0.8128097280639653\n",
      "training dataset accuracy for hidden layer sizes 25 , 75 , 10 : 0.8251403270229752\n",
      "testing dataset accuracy for hidden layer sizes 25 , 75 , 10 : 0.8268437929454878\n",
      "training dataset accuracy for hidden layer sizes 50 , 75 , 10 : 0.8326360562005369\n",
      "testing dataset accuracy for hidden layer sizes 50 , 75 , 10 : 0.8235122641902303\n",
      "training dataset accuracy for hidden layer sizes 75 , 75 , 10 : 0.8397221350625806\n",
      "testing dataset accuracy for hidden layer sizes 75 , 75 , 10 : 0.8213884146087536\n",
      "training dataset accuracy for hidden layer sizes 100 , 75 , 10 : 0.8411079733640136\n",
      "testing dataset accuracy for hidden layer sizes 100 , 75 , 10 : 0.828301336775913\n",
      "training dataset accuracy for hidden layer sizes 150 , 75 , 10 : 0.8463898476449465\n",
      "testing dataset accuracy for hidden layer sizes 150 , 75 , 10 : 0.8166826302419523\n",
      "training dataset accuracy for hidden layer sizes 200 , 75 , 10 : 0.8439668096084789\n",
      "testing dataset accuracy for hidden layer sizes 200 , 75 , 10 : 0.8210552617332278\n",
      "training dataset accuracy for hidden layer sizes 250 , 75 , 10 : 0.8520813722413974\n",
      "testing dataset accuracy for hidden layer sizes 250 , 75 , 10 : 0.8176404447590888\n",
      "training dataset accuracy for hidden layer sizes 5 , 100 , 10 : 0.8224035142767493\n",
      "testing dataset accuracy for hidden layer sizes 5 , 100 , 10 : 0.8192229209178362\n",
      "training dataset accuracy for hidden layer sizes 10 , 100 , 10 : 0.8212007112226755\n",
      "testing dataset accuracy for hidden layer sizes 10 , 100 , 10 : 0.8205971765293799\n",
      "training dataset accuracy for hidden layer sizes 25 , 100 , 10 : 0.822664993201548\n",
      "testing dataset accuracy for hidden layer sizes 25 , 100 , 10 : 0.8063548911006538\n",
      "training dataset accuracy for hidden layer sizes 50 , 100 , 10 : 0.8231356552661856\n",
      "testing dataset accuracy for hidden layer sizes 50 , 100 , 10 : 0.8213884146087536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 75 , 100 , 10 : 0.8381445455496287\n",
      "testing dataset accuracy for hidden layer sizes 75 , 100 , 10 : 0.8275933869154208\n",
      "training dataset accuracy for hidden layer sizes 100 , 100 , 10 : 0.8311107624725447\n",
      "testing dataset accuracy for hidden layer sizes 100 , 100 , 10 : 0.8247615874734519\n",
      "training dataset accuracy for hidden layer sizes 150 , 100 , 10 : 0.8377959069832305\n",
      "testing dataset accuracy for hidden layer sizes 150 , 100 , 10 : 0.8273851663682172\n",
      "training dataset accuracy for hidden layer sizes 200 , 100 , 10 : 0.8447773942753547\n",
      "testing dataset accuracy for hidden layer sizes 200 , 100 , 10 : 0.8213051263898722\n",
      "training dataset accuracy for hidden layer sizes 250 , 100 , 10 : 0.8482027681902172\n",
      "testing dataset accuracy for hidden layer sizes 250 , 100 , 10 : 0.810935743139133\n",
      "training dataset accuracy for hidden layer sizes 5 , 150 , 10 : 0.8185249102255692\n",
      "testing dataset accuracy for hidden layer sizes 5 , 150 , 10 : 0.8043559738474992\n",
      "training dataset accuracy for hidden layer sizes 10 , 150 , 10 : 0.8199891922044417\n",
      "testing dataset accuracy for hidden layer sizes 10 , 150 , 10 : 0.8127264398450839\n",
      "training dataset accuracy for hidden layer sizes 25 , 150 , 10 : 0.8311804901858244\n",
      "testing dataset accuracy for hidden layer sizes 25 , 150 , 10 : 0.8178486653062924\n",
      "training dataset accuracy for hidden layer sizes 50 , 150 , 10 : 0.8430167695150438\n",
      "testing dataset accuracy for hidden layer sizes 50 , 150 , 10 : 0.8191396326989547\n",
      "training dataset accuracy for hidden layer sizes 75 , 150 , 10 : 0.8381358295854687\n",
      "testing dataset accuracy for hidden layer sizes 75 , 150 , 10 : 0.8150585099737642\n",
      "training dataset accuracy for hidden layer sizes 100 , 150 , 10 : 0.8506693860474845\n",
      "testing dataset accuracy for hidden layer sizes 100 , 150 , 10 : 0.8237621288468746\n",
      "training dataset accuracy for hidden layer sizes 150 , 150 , 10 : 0.8509047170798034\n",
      "testing dataset accuracy for hidden layer sizes 150 , 150 , 10 : 0.8203473118727356\n",
      "training dataset accuracy for hidden layer sizes 200 , 150 , 10 : 0.8440191053934386\n",
      "testing dataset accuracy for hidden layer sizes 200 , 150 , 10 : 0.8265106400699621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 150 , 10 : 0.8586967890388034\n",
      "testing dataset accuracy for hidden layer sizes 250 , 150 , 10 : 0.8230125348769417\n",
      "training dataset accuracy for hidden layer sizes 5 , 200 , 10 : 0.8155789143395042\n",
      "testing dataset accuracy for hidden layer sizes 5 , 200 , 10 : 0.7954024903177446\n",
      "training dataset accuracy for hidden layer sizes 10 , 200 , 10 : 0.8227347209148276\n",
      "testing dataset accuracy for hidden layer sizes 10 , 200 , 10 : 0.8032732270020405\n",
      "training dataset accuracy for hidden layer sizes 25 , 200 , 10 : 0.8300212669525503\n",
      "testing dataset accuracy for hidden layer sizes 25 , 200 , 10 : 0.8210136176237871\n",
      "training dataset accuracy for hidden layer sizes 50 , 200 , 10 : 0.842188752919848\n",
      "testing dataset accuracy for hidden layer sizes 50 , 200 , 10 : 0.8205555324199392\n",
      "training dataset accuracy for hidden layer sizes 75 , 200 , 10 : 0.838867970574905\n",
      "testing dataset accuracy for hidden layer sizes 75 , 200 , 10 : 0.8182234622912589\n",
      "training dataset accuracy for hidden layer sizes 100 , 200 , 10 : 0.849222535996932\n",
      "testing dataset accuracy for hidden layer sizes 100 , 200 , 10 : 0.8188481239328697\n",
      "training dataset accuracy for hidden layer sizes 150 , 200 , 10 : 0.8530837081197922\n",
      "testing dataset accuracy for hidden layer sizes 150 , 200 , 10 : 0.8287594219797609\n",
      "training dataset accuracy for hidden layer sizes 200 , 200 , 10 : 0.843173656869923\n",
      "testing dataset accuracy for hidden layer sizes 200 , 200 , 10 : 0.8218048557031608\n",
      "training dataset accuracy for hidden layer sizes 250 , 200 , 10 : 0.8616079210682286\n",
      "testing dataset accuracy for hidden layer sizes 250 , 200 , 10 : 0.8152667305209678\n",
      "training dataset accuracy for hidden layer sizes 5 , 250 , 10 : 0.8203029669142\n",
      "testing dataset accuracy for hidden layer sizes 5 , 250 , 10 : 0.7943197434722858\n",
      "training dataset accuracy for hidden layer sizes 10 , 250 , 10 : 0.8227260049506676\n",
      "testing dataset accuracy for hidden layer sizes 10 , 250 , 10 : 0.8065214675384167\n",
      "training dataset accuracy for hidden layer sizes 25 , 250 , 10 : 0.8327842275912561\n",
      "testing dataset accuracy for hidden layer sizes 25 , 250 , 10 : 0.8182651064006996\n",
      "training dataset accuracy for hidden layer sizes 50 , 250 , 10 : 0.8265958930376879\n",
      "testing dataset accuracy for hidden layer sizes 50 , 250 , 10 : 0.8111023195768958\n",
      "training dataset accuracy for hidden layer sizes 75 , 250 , 10 : 0.8494578670292507\n",
      "testing dataset accuracy for hidden layer sizes 75 , 250 , 10 : 0.8143089160038313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 100 , 250 , 10 : 0.8550883798765819\n",
      "testing dataset accuracy for hidden layer sizes 100 , 250 , 10 : 0.817473868321326\n",
      "training dataset accuracy for hidden layer sizes 150 , 250 , 10 : 0.8323658613115783\n",
      "testing dataset accuracy for hidden layer sizes 150 , 250 , 10 : 0.8129763045017282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 250 , 10 : 0.8631244988320608\n",
      "testing dataset accuracy for hidden layer sizes 200 , 250 , 10 : 0.8142256277849498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 250 , 10 : 0.862096015061186\n",
      "testing dataset accuracy for hidden layer sizes 250 , 250 , 10 : 0.8128513721734061\n",
      "training dataset accuracy for hidden layer sizes 5 , 5 , 25 : 0.8156137781961441\n",
      "testing dataset accuracy for hidden layer sizes 5 , 5 , 25 : 0.8116020488901845\n",
      "training dataset accuracy for hidden layer sizes 10 , 5 , 25 : 0.821087403688596\n",
      "testing dataset accuracy for hidden layer sizes 10 , 5 , 25 : 0.8191812768083955\n",
      "training dataset accuracy for hidden layer sizes 25 , 5 , 25 : 0.8198846006345222\n",
      "testing dataset accuracy for hidden layer sizes 25 , 5 , 25 : 0.82696872527381\n",
      "training dataset accuracy for hidden layer sizes 50 , 5 , 25 : 0.8226039814524283\n",
      "testing dataset accuracy for hidden layer sizes 50 , 5 , 25 : 0.8142256277849498\n",
      "training dataset accuracy for hidden layer sizes 75 , 5 , 25 : 0.8309364431893456\n",
      "testing dataset accuracy for hidden layer sizes 75 , 5 , 25 : 0.8251363844584183\n",
      "training dataset accuracy for hidden layer sizes 100 , 5 , 25 : 0.8299689711675906\n",
      "testing dataset accuracy for hidden layer sizes 100 , 5 , 25 : 0.8267605047266064\n",
      "training dataset accuracy for hidden layer sizes 150 , 5 , 25 : 0.8333071854408535\n",
      "testing dataset accuracy for hidden layer sizes 150 , 5 , 25 : 0.8227210261108566\n",
      "training dataset accuracy for hidden layer sizes 200 , 5 , 25 : 0.832409441132378\n",
      "testing dataset accuracy for hidden layer sizes 200 , 5 , 25 : 0.8221796526881273\n",
      "training dataset accuracy for hidden layer sizes 250 , 5 , 25 : 0.8331241501934944\n",
      "testing dataset accuracy for hidden layer sizes 250 , 5 , 25 : 0.8143089160038313\n",
      "training dataset accuracy for hidden layer sizes 5 , 10 , 25 : 0.8156312101244639\n",
      "testing dataset accuracy for hidden layer sizes 5 , 10 , 25 : 0.8068962645233831\n",
      "training dataset accuracy for hidden layer sizes 10 , 10 , 25 : 0.818158839730851\n",
      "testing dataset accuracy for hidden layer sizes 10 , 10 , 25 : 0.8128513721734061\n",
      "training dataset accuracy for hidden layer sizes 25 , 10 , 25 : 0.8247742565282572\n",
      "testing dataset accuracy for hidden layer sizes 25 , 10 , 25 : 0.8187648357139883\n",
      "training dataset accuracy for hidden layer sizes 50 , 10 , 25 : 0.8252362026287348\n",
      "testing dataset accuracy for hidden layer sizes 50 , 10 , 25 : 0.8301753217007455\n",
      "training dataset accuracy for hidden layer sizes 75 , 10 , 25 : 0.825201338772095\n",
      "testing dataset accuracy for hidden layer sizes 75 , 10 , 25 : 0.8155165951776121\n",
      "training dataset accuracy for hidden layer sizes 100 , 10 , 25 : 0.8283390858696789\n",
      "testing dataset accuracy for hidden layer sizes 100 , 10 , 25 : 0.8111023195768958\n",
      "training dataset accuracy for hidden layer sizes 150 , 10 , 25 : 0.8314681170031029\n",
      "testing dataset accuracy for hidden layer sizes 150 , 10 , 25 : 0.8099362845125557\n",
      "training dataset accuracy for hidden layer sizes 200 , 10 , 25 : 0.8328365233762158\n",
      "testing dataset accuracy for hidden layer sizes 200 , 10 , 25 : 0.8096864198559114\n",
      "training dataset accuracy for hidden layer sizes 250 , 10 , 25 : 0.832418157096538\n",
      "testing dataset accuracy for hidden layer sizes 250 , 10 , 25 : 0.8195977179028027\n",
      "training dataset accuracy for hidden layer sizes 5 , 25 , 25 : 0.8150995363107066\n",
      "testing dataset accuracy for hidden layer sizes 5 , 25 , 25 : 0.7995669012618165\n",
      "training dataset accuracy for hidden layer sizes 10 , 25 , 25 : 0.8232925426210648\n",
      "testing dataset accuracy for hidden layer sizes 10 , 25 , 25 : 0.8216799233748386\n",
      "training dataset accuracy for hidden layer sizes 25 , 25 , 25 : 0.8271972945647248\n",
      "testing dataset accuracy for hidden layer sizes 25 , 25 , 25 : 0.8204306000916171\n",
      "training dataset accuracy for hidden layer sizes 50 , 25 , 25 : 0.825584841195133\n",
      "testing dataset accuracy for hidden layer sizes 50 , 25 , 25 : 0.8154333069587307\n",
      "training dataset accuracy for hidden layer sizes 75 , 25 , 25 : 0.8311107624725447\n",
      "testing dataset accuracy for hidden layer sizes 75 , 25 , 25 : 0.8287177778703202\n",
      "training dataset accuracy for hidden layer sizes 100 , 25 , 25 : 0.8356256319074016\n",
      "testing dataset accuracy for hidden layer sizes 100 , 25 , 25 : 0.8248032315828926\n",
      "training dataset accuracy for hidden layer sizes 150 , 25 , 25 : 0.8390945856430638\n",
      "testing dataset accuracy for hidden layer sizes 150 , 25 , 25 : 0.8206388206388207\n",
      "training dataset accuracy for hidden layer sizes 200 , 25 , 25 : 0.848446815186696\n",
      "testing dataset accuracy for hidden layer sizes 200 , 25 , 25 : 0.8254695373339441\n",
      "training dataset accuracy for hidden layer sizes 250 , 25 , 25 : 0.8324443049890179\n",
      "testing dataset accuracy for hidden layer sizes 250 , 25 , 25 : 0.8113938283429809\n",
      "training dataset accuracy for hidden layer sizes 5 , 50 , 25 : 0.8161018721891016\n",
      "testing dataset accuracy for hidden layer sizes 5 , 50 , 25 : 0.8050222795985508\n",
      "training dataset accuracy for hidden layer sizes 10 , 50 , 25 : 0.8224296621692292\n",
      "testing dataset accuracy for hidden layer sizes 10 , 50 , 25 : 0.8259692666472328\n",
      "training dataset accuracy for hidden layer sizes 25 , 50 , 25 : 0.8315378447163825\n",
      "testing dataset accuracy for hidden layer sizes 25 , 50 , 25 : 0.8223462291258902\n",
      "training dataset accuracy for hidden layer sizes 50 , 50 , 25 : 0.8321915420283792\n",
      "testing dataset accuracy for hidden layer sizes 50 , 50 , 25 : 0.8267188606171657\n",
      "training dataset accuracy for hidden layer sizes 75 , 50 , 25 : 0.8330369905518948\n",
      "testing dataset accuracy for hidden layer sizes 75 , 50 , 25 : 0.8163078332569857\n",
      "training dataset accuracy for hidden layer sizes 100 , 50 , 25 : 0.8348586270613255\n",
      "testing dataset accuracy for hidden layer sizes 100 , 50 , 25 : 0.814933577645442\n",
      "training dataset accuracy for hidden layer sizes 150 , 50 , 25 : 0.8338911550395705\n",
      "testing dataset accuracy for hidden layer sizes 150 , 50 , 25 : 0.8174322242118852\n",
      "training dataset accuracy for hidden layer sizes 200 , 50 , 25 : 0.8435571592929609\n",
      "testing dataset accuracy for hidden layer sizes 200 , 50 , 25 : 0.8246366551451297\n",
      "training dataset accuracy for hidden layer sizes 250 , 50 , 25 : 0.8486559983265349\n",
      "testing dataset accuracy for hidden layer sizes 250 , 50 , 25 : 0.8173905801024445\n",
      "training dataset accuracy for hidden layer sizes 5 , 75 , 25 : 0.8194139385698846\n",
      "testing dataset accuracy for hidden layer sizes 5 , 75 , 25 : 0.8069795527422646\n",
      "training dataset accuracy for hidden layer sizes 10 , 75 , 25 : 0.8212442910434752\n",
      "testing dataset accuracy for hidden layer sizes 10 , 75 , 25 : 0.8160163244909008\n",
      "training dataset accuracy for hidden layer sizes 25 , 75 , 25 : 0.8314681170031029\n",
      "testing dataset accuracy for hidden layer sizes 25 , 75 , 25 : 0.8205971765293799\n",
      "training dataset accuracy for hidden layer sizes 50 , 75 , 25 : 0.8264913014677684\n",
      "testing dataset accuracy for hidden layer sizes 50 , 75 , 25 : 0.8217632115937201\n",
      "training dataset accuracy for hidden layer sizes 75 , 75 , 25 : 0.8347278875989262\n",
      "testing dataset accuracy for hidden layer sizes 75 , 75 , 25 : 0.8144338483321534\n",
      "training dataset accuracy for hidden layer sizes 100 , 75 , 25 : 0.8484903950074957\n",
      "testing dataset accuracy for hidden layer sizes 100 , 75 , 25 : 0.8160996127097822\n",
      "training dataset accuracy for hidden layer sizes 150 , 75 , 25 : 0.846293972039187\n",
      "testing dataset accuracy for hidden layer sizes 150 , 75 , 25 : 0.8232623995335859\n",
      "training dataset accuracy for hidden layer sizes 200 , 75 , 25 : 0.8473486037025416\n",
      "testing dataset accuracy for hidden layer sizes 200 , 75 , 25 : 0.8199308707783284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 75 , 25 : 0.8567705609594534\n",
      "testing dataset accuracy for hidden layer sizes 250 , 75 , 25 : 0.8173489359930038\n",
      "training dataset accuracy for hidden layer sizes 5 , 100 , 25 : 0.8203465467349998\n",
      "testing dataset accuracy for hidden layer sizes 5 , 100 , 25 : 0.7918210969058427\n",
      "training dataset accuracy for hidden layer sizes 10 , 100 , 25 : 0.8235627375100234\n",
      "testing dataset accuracy for hidden layer sizes 10 , 100 , 25 : 0.824011993503519\n",
      "training dataset accuracy for hidden layer sizes 25 , 100 , 25 : 0.8273977617404037\n",
      "testing dataset accuracy for hidden layer sizes 25 , 100 , 25 : 0.8143922042227127\n",
      "training dataset accuracy for hidden layer sizes 50 , 100 , 25 : 0.8407244709409755\n",
      "testing dataset accuracy for hidden layer sizes 50 , 100 , 25 : 0.8148502894265606\n",
      "training dataset accuracy for hidden layer sizes 75 , 100 , 25 : 0.848237632046857\n",
      "testing dataset accuracy for hidden layer sizes 75 , 100 , 25 : 0.8268854370549286\n",
      "training dataset accuracy for hidden layer sizes 100 , 100 , 25 : 0.847453195272461\n",
      "testing dataset accuracy for hidden layer sizes 100 , 100 , 25 : 0.8254278932245034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 150 , 100 , 25 : 0.8543475229229858\n",
      "testing dataset accuracy for hidden layer sizes 150 , 100 , 25 : 0.8214300587181943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 100 , 25 : 0.8571889272391312\n",
      "testing dataset accuracy for hidden layer sizes 200 , 100 , 25 : 0.8198892266688876\n",
      "training dataset accuracy for hidden layer sizes 250 , 100 , 25 : 0.854286511173866\n",
      "testing dataset accuracy for hidden layer sizes 250 , 100 , 25 : 0.8159330362720193\n",
      "training dataset accuracy for hidden layer sizes 5 , 150 , 25 : 0.8203465467349998\n",
      "testing dataset accuracy for hidden layer sizes 5 , 150 , 25 : 0.8019822596093783\n",
      "training dataset accuracy for hidden layer sizes 10 , 150 , 25 : 0.8226824251298679\n",
      "testing dataset accuracy for hidden layer sizes 10 , 150 , 25 : 0.8106442343730479\n",
      "training dataset accuracy for hidden layer sizes 25 , 150 , 25 : 0.8305355088379877\n",
      "testing dataset accuracy for hidden layer sizes 25 , 150 , 25 : 0.82638570774164\n",
      "training dataset accuracy for hidden layer sizes 50 , 150 , 25 : 0.8363577728968379\n",
      "testing dataset accuracy for hidden layer sizes 50 , 150 , 25 : 0.824011993503519\n",
      "training dataset accuracy for hidden layer sizes 75 , 150 , 25 : 0.8387197991841857\n",
      "testing dataset accuracy for hidden layer sizes 75 , 150 , 25 : 0.8176820888685296\n",
      "training dataset accuracy for hidden layer sizes 100 , 150 , 25 : 0.8512533556462016\n",
      "testing dataset accuracy for hidden layer sizes 100 , 150 , 25 : 0.8218881439220422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 150 , 150 , 25 : 0.8574852700205696\n",
      "testing dataset accuracy for hidden layer sizes 150 , 150 , 25 : 0.8144338483321534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 150 , 25 : 0.8590454276052016\n",
      "testing dataset accuracy for hidden layer sizes 200 , 150 , 25 : 0.820888685295465\n",
      "training dataset accuracy for hidden layer sizes 250 , 150 , 25 : 0.8565613778196144\n",
      "testing dataset accuracy for hidden layer sizes 250 , 150 , 25 : 0.8158497480531379\n",
      "training dataset accuracy for hidden layer sizes 5 , 200 , 25 : 0.8221420353519506\n",
      "testing dataset accuracy for hidden layer sizes 5 , 200 , 25 : 0.811518760671303\n",
      "training dataset accuracy for hidden layer sizes 10 , 200 , 25 : 0.8231966670153053\n",
      "testing dataset accuracy for hidden layer sizes 10 , 200 , 25 : 0.8069379086328239\n",
      "training dataset accuracy for hidden layer sizes 25 , 200 , 25 : 0.8365756720008367\n",
      "testing dataset accuracy for hidden layer sizes 25 , 200 , 25 : 0.8230958230958231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 50 , 200 , 25 : 0.8480894606561378\n",
      "testing dataset accuracy for hidden layer sizes 50 , 200 , 25 : 0.818806479823429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 75 , 200 , 25 : 0.8516717219258795\n",
      "testing dataset accuracy for hidden layer sizes 75 , 200 , 25 : 0.8165576979136301\n",
      "training dataset accuracy for hidden layer sizes 100 , 200 , 25 : 0.8495363107066903\n",
      "testing dataset accuracy for hidden layer sizes 100 , 200 , 25 : 0.8204306000916171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 150 , 200 , 25 : 0.8623313460935048\n",
      "testing dataset accuracy for hidden layer sizes 150 , 200 , 25 : 0.8136426102527797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 200 , 25 : 0.8635341491475788\n",
      "testing dataset accuracy for hidden layer sizes 200 , 200 , 25 : 0.814642068879357\n",
      "training dataset accuracy for hidden layer sizes 250 , 200 , 25 : 0.8623313460935048\n",
      "testing dataset accuracy for hidden layer sizes 250 , 200 , 25 : 0.8185149710573439\n",
      "training dataset accuracy for hidden layer sizes 5 , 250 , 25 : 0.8206864693372381\n",
      "testing dataset accuracy for hidden layer sizes 5 , 250 , 25 : 0.7938200141589972\n",
      "training dataset accuracy for hidden layer sizes 10 , 250 , 25 : 0.8265435972527281\n",
      "testing dataset accuracy for hidden layer sizes 10 , 250 , 25 : 0.8139757631283056\n",
      "training dataset accuracy for hidden layer sizes 25 , 250 , 25 : 0.8344141128891678\n",
      "testing dataset accuracy for hidden layer sizes 25 , 250 , 25 : 0.8198059384500063\n",
      "training dataset accuracy for hidden layer sizes 50 , 250 , 25 : 0.8506781020116445\n",
      "testing dataset accuracy for hidden layer sizes 50 , 250 , 25 : 0.8264273518510806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 75 , 250 , 25 : 0.8523689990586759\n",
      "testing dataset accuracy for hidden layer sizes 75 , 250 , 25 : 0.8153500187398492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 100 , 250 , 25 : 0.8558728166509779\n",
      "testing dataset accuracy for hidden layer sizes 100 , 250 , 25 : 0.8086453171198934\n",
      "training dataset accuracy for hidden layer sizes 150 , 250 , 25 : 0.8567792769236133\n",
      "testing dataset accuracy for hidden layer sizes 150 , 250 , 25 : 0.8240536376129597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 250 , 25 : 0.8611982707527107\n",
      "testing dataset accuracy for hidden layer sizes 200 , 250 , 25 : 0.805147211926873\n",
      "training dataset accuracy for hidden layer sizes 250 , 250 , 25 : 0.8575898615904891\n",
      "testing dataset accuracy for hidden layer sizes 250 , 250 , 25 : 0.8134760338150169\n",
      "training dataset accuracy for hidden layer sizes 5 , 5 , 50 : 0.8157532336227034\n",
      "testing dataset accuracy for hidden layer sizes 5 , 5 , 50 : 0.8156415275059343\n",
      "training dataset accuracy for hidden layer sizes 10 , 5 , 50 : 0.8210699717602761\n",
      "testing dataset accuracy for hidden layer sizes 10 , 5 , 50 : 0.8128097280639653\n",
      "training dataset accuracy for hidden layer sizes 25 , 5 , 50 : 0.8210002440469965\n",
      "testing dataset accuracy for hidden layer sizes 25 , 5 , 50 : 0.8093116228709449\n",
      "training dataset accuracy for hidden layer sizes 50 , 5 , 50 : 0.8233274064777045\n",
      "testing dataset accuracy for hidden layer sizes 50 , 5 , 50 : 0.8145171365510349\n",
      "training dataset accuracy for hidden layer sizes 75 , 5 , 50 : 0.8272321584213645\n",
      "testing dataset accuracy for hidden layer sizes 75 , 5 , 50 : 0.808270520134927\n",
      "training dataset accuracy for hidden layer sizes 100 , 5 , 50 : 0.8275982289160827\n",
      "testing dataset accuracy for hidden layer sizes 100 , 5 , 50 : 0.8148919335360013\n",
      "training dataset accuracy for hidden layer sizes 150 , 5 , 50 : 0.8356953596206812\n",
      "testing dataset accuracy for hidden layer sizes 150 , 5 , 50 : 0.8246782992545705\n",
      "training dataset accuracy for hidden layer sizes 200 , 5 , 50 : 0.8351113900219642\n",
      "testing dataset accuracy for hidden layer sizes 200 , 5 , 50 : 0.8148919335360013\n",
      "training dataset accuracy for hidden layer sizes 250 , 5 , 50 : 0.8353118571976432\n",
      "testing dataset accuracy for hidden layer sizes 250 , 5 , 50 : 0.8253029608961813\n",
      "training dataset accuracy for hidden layer sizes 5 , 10 , 50 : 0.8153174354147056\n",
      "testing dataset accuracy for hidden layer sizes 5 , 10 , 50 : 0.8113521842335402\n",
      "training dataset accuracy for hidden layer sizes 10 , 10 , 50 : 0.8199543283478018\n",
      "testing dataset accuracy for hidden layer sizes 10 , 10 , 50 : 0.8214300587181943\n",
      "training dataset accuracy for hidden layer sizes 25 , 10 , 50 : 0.8242338667503399\n",
      "testing dataset accuracy for hidden layer sizes 25 , 10 , 50 : 0.8118935576562695\n",
      "training dataset accuracy for hidden layer sizes 50 , 10 , 50 : 0.8255238294460133\n",
      "testing dataset accuracy for hidden layer sizes 50 , 10 , 50 : 0.8146837129887977\n",
      "training dataset accuracy for hidden layer sizes 75 , 10 , 50 : 0.8280427430882404\n",
      "testing dataset accuracy for hidden layer sizes 75 , 10 , 50 : 0.8129763045017282\n",
      "training dataset accuracy for hidden layer sizes 100 , 10 , 50 : 0.8327842275912561\n",
      "testing dataset accuracy for hidden layer sizes 100 , 10 , 50 : 0.8130595927206097\n",
      "training dataset accuracy for hidden layer sizes 150 , 10 , 50 : 0.8332548896558938\n",
      "testing dataset accuracy for hidden layer sizes 150 , 10 , 50 : 0.8260525548661142\n",
      "training dataset accuracy for hidden layer sizes 200 , 10 , 50 : 0.83984415856082\n",
      "testing dataset accuracy for hidden layer sizes 200 , 10 , 50 : 0.8289259984175238\n",
      "training dataset accuracy for hidden layer sizes 250 , 10 , 50 : 0.8297597880277516\n",
      "testing dataset accuracy for hidden layer sizes 250 , 10 , 50 : 0.8228043143297381\n",
      "training dataset accuracy for hidden layer sizes 5 , 25 , 50 : 0.8157445176585434\n",
      "testing dataset accuracy for hidden layer sizes 5 , 25 , 50 : 0.8096031316370299\n",
      "training dataset accuracy for hidden layer sizes 10 , 25 , 50 : 0.8221943311369103\n",
      "testing dataset accuracy for hidden layer sizes 10 , 25 , 50 : 0.8088118935576563\n",
      "training dataset accuracy for hidden layer sizes 25 , 25 , 50 : 0.8222553428860301\n",
      "testing dataset accuracy for hidden layer sizes 25 , 25 , 50 : 0.8014825302960896\n",
      "training dataset accuracy for hidden layer sizes 50 , 25 , 50 : 0.8302217341282293\n",
      "testing dataset accuracy for hidden layer sizes 50 , 25 , 50 : 0.8202223795444135\n",
      "training dataset accuracy for hidden layer sizes 75 , 25 , 50 : 0.8284698253320782\n",
      "testing dataset accuracy for hidden layer sizes 75 , 25 , 50 : 0.8013575979677675\n",
      "training dataset accuracy for hidden layer sizes 100 , 25 , 50 : 0.8397482829550605\n",
      "testing dataset accuracy for hidden layer sizes 100 , 25 , 50 : 0.8253446050056219\n",
      "training dataset accuracy for hidden layer sizes 150 , 25 , 50 : 0.8293152738555939\n",
      "testing dataset accuracy for hidden layer sizes 150 , 25 , 50 : 0.8258026902094698\n",
      "training dataset accuracy for hidden layer sizes 200 , 25 , 50 : 0.8315204127880627\n",
      "testing dataset accuracy for hidden layer sizes 200 , 25 , 50 : 0.8250947403489777\n",
      "training dataset accuracy for hidden layer sizes 250 , 25 , 50 : 0.8344053969250078\n",
      "testing dataset accuracy for hidden layer sizes 250 , 25 , 50 : 0.8091866905426227\n",
      "training dataset accuracy for hidden layer sizes 5 , 50 , 50 : 0.8207300491580378\n",
      "testing dataset accuracy for hidden layer sizes 5 , 50 , 50 : 0.805147211926873\n",
      "training dataset accuracy for hidden layer sizes 10 , 50 , 50 : 0.824408186033539\n",
      "testing dataset accuracy for hidden layer sizes 10 , 50 , 50 : 0.8069795527422646\n",
      "training dataset accuracy for hidden layer sizes 25 , 50 , 50 : 0.8263954258620089\n",
      "testing dataset accuracy for hidden layer sizes 25 , 50 , 50 : 0.8217632115937201\n",
      "training dataset accuracy for hidden layer sizes 50 , 50 , 50 : 0.8281037548373601\n",
      "testing dataset accuracy for hidden layer sizes 50 , 50 , 50 : 0.8178486653062924\n",
      "training dataset accuracy for hidden layer sizes 75 , 50 , 50 : 0.8371596415995537\n",
      "testing dataset accuracy for hidden layer sizes 75 , 50 , 50 : 0.8081872319160455\n",
      "training dataset accuracy for hidden layer sizes 100 , 50 , 50 : 0.8332374577275738\n",
      "testing dataset accuracy for hidden layer sizes 100 , 50 , 50 : 0.8228459584391787\n",
      "training dataset accuracy for hidden layer sizes 150 , 50 , 50 : 0.8449691454868737\n",
      "testing dataset accuracy for hidden layer sizes 150 , 50 , 50 : 0.812559863407321\n",
      "training dataset accuracy for hidden layer sizes 200 , 50 , 50 : 0.8371422096712339\n",
      "testing dataset accuracy for hidden layer sizes 200 , 50 , 50 : 0.8159330362720193\n",
      "training dataset accuracy for hidden layer sizes 250 , 50 , 50 : 0.8532405954746715\n",
      "testing dataset accuracy for hidden layer sizes 250 , 50 , 50 : 0.816432765585308\n",
      "training dataset accuracy for hidden layer sizes 5 , 75 , 50 : 0.8216452253948332\n",
      "testing dataset accuracy for hidden layer sizes 5 , 75 , 50 : 0.8111439636863366\n",
      "training dataset accuracy for hidden layer sizes 10 , 75 , 50 : 0.8229700519471463\n",
      "testing dataset accuracy for hidden layer sizes 10 , 75 , 50 : 0.8116853371090659\n",
      "training dataset accuracy for hidden layer sizes 25 , 75 , 50 : 0.8344141128891678\n",
      "testing dataset accuracy for hidden layer sizes 25 , 75 , 50 : 0.8302169658101861\n",
      "training dataset accuracy for hidden layer sizes 50 , 75 , 50 : 0.8257591604783321\n",
      "testing dataset accuracy for hidden layer sizes 50 , 75 , 50 : 0.8066880439761795\n",
      "training dataset accuracy for hidden layer sizes 75 , 75 , 50 : 0.8385106160443468\n",
      "testing dataset accuracy for hidden layer sizes 75 , 75 , 50 : 0.8157664598342564\n",
      "training dataset accuracy for hidden layer sizes 100 , 75 , 50 : 0.8464072795732664\n",
      "testing dataset accuracy for hidden layer sizes 100 , 75 , 50 : 0.82118019406155\n",
      "training dataset accuracy for hidden layer sizes 150 , 75 , 50 : 0.8510093086497228\n",
      "testing dataset accuracy for hidden layer sizes 150 , 75 , 50 : 0.8234706200807895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 200 , 75 , 50 : 0.8571889272391312\n",
      "testing dataset accuracy for hidden layer sizes 200 , 75 , 50 : 0.8216382792653979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 250 , 75 , 50 : 0.8579472161210473\n",
      "testing dataset accuracy for hidden layer sizes 250 , 75 , 50 : 0.8250530962395369\n",
      "training dataset accuracy for hidden layer sizes 5 , 100 , 50 : 0.821279154900115\n",
      "testing dataset accuracy for hidden layer sizes 5 , 100 , 50 : 0.7973181193520177\n",
      "training dataset accuracy for hidden layer sizes 10 , 100 , 50 : 0.8242512986786599\n",
      "testing dataset accuracy for hidden layer sizes 10 , 100 , 50 : 0.8153500187398492\n",
      "training dataset accuracy for hidden layer sizes 25 , 100 , 50 : 0.8284175295471184\n",
      "testing dataset accuracy for hidden layer sizes 25 , 100 , 50 : 0.8173072918835631\n",
      "training dataset accuracy for hidden layer sizes 50 , 100 , 50 : 0.8390074260014643\n",
      "testing dataset accuracy for hidden layer sizes 50 , 100 , 50 : 0.8148086453171199\n",
      "training dataset accuracy for hidden layer sizes 75 , 100 , 50 : 0.8448122581319946\n",
      "testing dataset accuracy for hidden layer sizes 75 , 100 , 50 : 0.818806479823429\n",
      "training dataset accuracy for hidden layer sizes 100 , 100 , 50 : 0.8512707875745215\n",
      "testing dataset accuracy for hidden layer sizes 100 , 100 , 50 : 0.8207637529671428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for hidden layer sizes 150 , 100 , 50 : 0.8581302513684064\n",
      "testing dataset accuracy for hidden layer sizes 150 , 100 , 50 : 0.8268854370549286\n",
      "training dataset accuracy for hidden layer sizes 200 , 100 , 50 : 0.8576508733396089\n",
      "testing dataset accuracy for hidden layer sizes 200 , 100 , 50 : 0.8192645650272768\n",
      "training dataset accuracy for hidden layer sizes 250 , 100 , 50 : 0.8573981103789701\n",
      "testing dataset accuracy for hidden layer sizes 250 , 100 , 50 : 0.8170574272269188\n",
      "training dataset accuracy for hidden layer sizes 5 , 150 , 50 : 0.8227260049506676\n",
      "testing dataset accuracy for hidden layer sizes 5 , 150 , 50 : 0.8068962645233831\n",
      "training dataset accuracy for hidden layer sizes 10 , 150 , 50 : 0.8223076386709898\n",
      "testing dataset accuracy for hidden layer sizes 10 , 150 , 50 : 0.8117686253279474\n",
      "training dataset accuracy for hidden layer sizes 25 , 150 , 50 : 0.8311020465083847\n",
      "testing dataset accuracy for hidden layer sizes 25 , 150 , 50 : 0.8116853371090659\n"
     ]
    }
   ],
   "source": [
    "for eachhn in num:\n",
    "    for eachn in num:\n",
    "        for numhiddenunit in num:\n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(numhiddenunit,eachn,eachhn), random_state=1,alpha=0.00001)\n",
    "            classifier.fit(X_trainnn, y_trainnn.ravel())\n",
    "            nntrain_result = classifier.predict(X_trainnn)\n",
    "            nntest_result = classifier.predict(X_testnn)\n",
    "            print ('training dataset accuracy for hidden layer sizes',numhiddenunit,',',eachn,',',eachhn,':',accuracy_score(y_trainnn, nntrain_result))\n",
    "            print ('testing dataset accuracy for hidden layer sizes',numhiddenunit,',',eachn,',',eachhn,':',accuracy_score(y_testnn, nntest_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-06, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200, 200, 25), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=400, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data\n",
    "nnclf = MLPClassifier(hidden_layer_sizes=(200,200,25), random_state=1,alpha=1e-6,max_iter=400)\n",
    "nnclf.fit(X_trainnn, y_trainnn.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training dataset\n",
      "training dataset accuracy: 0.8687811595718719\n",
      "\n",
      "\n",
      "Classification report for classifier MLPClassifier(activation='relu', alpha=1e-06, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(200, 200, 25), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=400, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.84      0.86     57366\n",
      "          1       0.85      0.90      0.87     57366\n",
      "\n",
      "avg / total       0.87      0.87      0.87    114732\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[48151  9215]\n",
      " [ 5840 51526]]\n"
     ]
    }
   ],
   "source": [
    "#training dataset classfication report\n",
    "print('For training dataset')\n",
    "nntrain_result = nnclf.predict(X_trainnn)\n",
    "print ('training dataset accuracy: {}'.format(accuracy_score(y_trainnn, nntrain_result)))\n",
    "print('\\n')\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (nnclf, metrics.classification_report(y_trainnn, nntrain_result)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_trainnn, nntrain_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing dataset\n",
      "testing dataset accuracy: 0.8168908507891559\n",
      "\n",
      "\n",
      "Classification report for classifier MLPClassifier(activation='relu', alpha=1e-06, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(200, 200, 25), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=400, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.63      0.58      4902\n",
      "          1       0.90      0.86      0.88     19111\n",
      "\n",
      "avg / total       0.83      0.82      0.82     24013\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 3096  1806]\n",
      " [ 2591 16520]]\n"
     ]
    }
   ],
   "source": [
    "#testing dataset classfication report\n",
    "print('For testing dataset')\n",
    "tnntrain_result = nnclf.predict(X_testnn)\n",
    "print ('testing dataset accuracy: {}'.format(accuracy_score(y_testnn, tnntrain_result)))\n",
    "print('\\n')\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (nnclf, metrics.classification_report(y_testnn, tnntrain_result)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_testnn, tnntrain_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lcyy\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainrf = np.array(y_train)\n",
    "y_testrf = np.array(y_test)\n",
    "X_trainrf = X_train\n",
    "X_testrf = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [25,50,100,150]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=ntree,) # same setting as orginal single decision tree\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for number of trees used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [80,90,100,110,120,130]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, class_weight=\"balanced\", max_features = None, n_estimators=ntree) # same setting as orginal single decision tree\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for number of trees used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [60,70,80,130,140]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, class_weight=\"balanced\", max_features = None, n_estimators=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for number of trees used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of trees used=80 has the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [50,100,250,500]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for maximun depth of the tree used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [60,70,80,90]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for maximun depth of the tree used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [61,62,63,64,65,66,67,68,69]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for maximun depth of the tree used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the maximun depth of the tree used=63 has the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [50,100,250,500]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None, class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=ntree) # same setting as orginal single decision tree\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for the minimum number of samples required to be at a leaf node used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [10,20,30,40]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None, class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=ntree) # same setting as orginal single decision tree\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for the minimum number of samples required to be at a leaf node used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [1,2,3,4,5]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None, class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=ntree) # same setting as orginal single decision tree\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for the minimum number of samples required to be at a leaf node used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the minimum number of samples required to be at a leaf node used=2 has the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [50,100,250,500]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None, class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2, min_samples_split=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for the minimum number of samples required to split an internal node used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [10,20,30,40]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None, class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2, min_samples_split=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for the minimum number of samples required to split an internal node used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntree in [2,3,4,5]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None, class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2, min_samples_split=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for the minimum number of samples required to split an internal node used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for the minimum number of samples required to split an internal node used 6: 0.8626577270645067\n",
      "training dataset accuracy for the minimum number of samples required to split an internal node used 7: 0.8624495065173031\n",
      "training dataset accuracy for the minimum number of samples required to split an internal node used 8: 0.8621996418606588\n",
      "training dataset accuracy for the minimum number of samples required to split an internal node used 9: 0.8619497772040144\n"
     ]
    }
   ],
   "source": [
    "for ntree in [6,7,8,9]:\n",
    "    clf2 = RandomForestClassifier(random_state=0, max_features = None, class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2, min_samples_split=ntree)\n",
    "    rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "    rf_result = rf_model.predict(X_testrf)\n",
    "    print ('training dataset accuracy for the minimum number of samples required to split an internal node used {}:'.format(ntree),accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the the minimum number of samples required to split an internal node used=5 has the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for not using bootstrap: 0.7538000249864657\n"
     ]
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2,min_samples_split=5, bootstrap=False)\n",
    "rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "rf_result = rf_model.predict(X_testrf)\n",
    "print ('training dataset accuracy for not using bootstrap:',accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for not using bootstrap: 0.7538000249864657\n"
     ]
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2,min_samples_split=5, bootstrap=False)\n",
    "rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "rf_result = rf_model.predict(X_testrf)\n",
    "print ('training dataset accuracy for not using bootstrap:',accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using bootstrap has a higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for using gini as criterion: 0.8628243035022696\n"
     ]
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2,min_samples_split=5, bootstrap=True,criterion=\"gini\" )\n",
    "rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "rf_result = rf_model.predict(X_testrf)\n",
    "print ('training dataset accuracy for using gini as criterion:',accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset accuracy for using entropy as criterion: 0.862616082955066\n"
     ]
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2,min_samples_split=5, bootstrap=True,criterion=\"entropy\" )\n",
    "rf_model = clf2.fit(X_trainrf, y_trainrf.ravel())\n",
    "rf_result = rf_model.predict(X_testrf)\n",
    "print ('training dataset accuracy for using entropy as criterion:',accuracy_score(y_testrf, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using gini and entropy as criterion have similar accuracy, so we decide to check the precision and recall to see which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IVAN YIP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2,min_samples_split=5, bootstrap=True,criterion=\"entropy\")\n",
    "RF = clf.fit(X_trainrf, y_trainrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test dataset\n",
      "training dataset accuracy: 0.862616082955066\n",
      "\n",
      "\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='entropy', max_depth=63, max_features=None,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=2,\n",
      "            min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=80, n_jobs=None, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.55      0.62      4902\n",
      "           1       0.89      0.94      0.92     19111\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     24013\n",
      "   macro avg       0.80      0.75      0.77     24013\n",
      "weighted avg       0.85      0.86      0.86     24013\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2684  2218]\n",
      " [ 1081 18030]]\n"
     ]
    }
   ],
   "source": [
    "#test dataset\n",
    "print('For test dataset')\n",
    "train_result = RF.predict(X_testrf)\n",
    "print ('training dataset accuracy: {}'.format(accuracy_score(y_testrf, train_result)))\n",
    "print('\\n')\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (RF, metrics.classification_report(y_testrf, train_result)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_testrf, train_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IVAN YIP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf_2 = RandomForestClassifier(random_state=0, max_features = None,class_weight=\"balanced\", n_estimators=80, max_depth=63, min_samples_leaf=2,min_samples_split=5, bootstrap=True,criterion=\"gini\")\n",
    "RF = clf_2.fit(X_trainrf, y_trainrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test dataset\n",
      "training dataset accuracy: 0.8628243035022696\n",
      "\n",
      "\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=63, max_features=None,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=2,\n",
      "            min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=80, n_jobs=None, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.53      0.61      4902\n",
      "           1       0.89      0.95      0.92     19111\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     24013\n",
      "   macro avg       0.81      0.74      0.76     24013\n",
      "weighted avg       0.85      0.86      0.85     24013\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2609  2293]\n",
      " [ 1001 18110]]\n"
     ]
    }
   ],
   "source": [
    "#test dataset\n",
    "print('For test dataset')\n",
    "train_result = RF.predict(X_testrf)\n",
    "print ('training dataset accuracy: {}'.format(accuracy_score(y_testrf, train_result)))\n",
    "print('\\n')\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (RF, metrics.classification_report(y_testrf, train_result)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_testrf, train_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = clf.predict_proba(X_testrf)\n",
    "predicted = clf.predict(X_testrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predict_probabilities = probability[:,1]\n",
    "fpr, tpr, _ = roc_curve(y_testrf, y_predict_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
